{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classificação do Fashion MNIST com NN simples e Regressão Logística"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Funções das Redes Neurais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "'''Criando a funcao de normalização de um dataframe inteiro\n",
    "    input:\n",
    "        df: Dataframe\n",
    "    output:\n",
    "        df: Dataframe com valores normalizados\n",
    "'''\n",
    "def normalize_dataframe(df):\n",
    "    for column in df:\n",
    "        df[column] = df[column]/255\n",
    "    return df\n",
    "\n",
    "\n",
    "'''Funcao para aplicar relu em um vetor\n",
    "    input:\n",
    "        z: weights * params + bias\n",
    "    output:\n",
    "        s: 0 para valores negativos z para valores positivos aplicado em cada amostra\n",
    "'''    \n",
    "def relu(z):\n",
    "    return np.maximum(z, 0)\n",
    "\n",
    "\n",
    "'''Funcao para aplicar a derivada da relu em um vetor\n",
    "    input:\n",
    "        z: weights * params + bias\n",
    "    output:\n",
    "        s: 0 para valores negativos 1 para valores positivos aplicado em cada amostra\n",
    "'''    \n",
    "def relu_(z):\n",
    "    return np.array(z>0).astype(int)\n",
    "\n",
    "\n",
    "'''Funcao para aplicar sigmoid em um vetor\n",
    "    input:\n",
    "        z: weights * params + bias\n",
    "    output:\n",
    "        s: valor entre 0-1 da aplicação da função para cada amostra\n",
    "'''\n",
    "def sigmoid(z):\n",
    "    #print(z.shape)\n",
    "    s = 1 / (1 + np.exp(-z))\n",
    "    return s\n",
    "\n",
    "\n",
    "'''Funcao para aplicar a derivada da sigmoid em um vetor\n",
    "    input:\n",
    "        z: weights * params + bias\n",
    "    output:\n",
    "        p: taxa de variação da sigmoid para cada amostra\n",
    "'''\n",
    "def sigmoid_(z):\n",
    "    #print(sigmoid(Z1).shape)\n",
    "    s = sigmoid(z) * (1 - sigmoid(z))\n",
    "    return s\n",
    "\n",
    "\n",
    "'''Funcao para calcular a tangente hiporbólica em um vetor\n",
    "    input:\n",
    "        x: weights * params + bias\n",
    "    output:\n",
    "        s: valor entre 0-1 da aplicação da função para cada amostra\n",
    "'''\n",
    "def tanh(x):\n",
    "    s = (2 / (1 + np.exp(-2*x))) -1\n",
    "    return s\n",
    "\n",
    "\n",
    "'''Funcao para calcular a derivada da tangente hiporbólica em um vetor\n",
    "    input:\n",
    "        x: weights * params + bias\n",
    "    output:\n",
    "        s: porcentagem de chance do valor ser da classe predita para cada amostra\n",
    "'''\n",
    "def tanh_(x):\n",
    "    s = 1 - tanh(x)**2\n",
    "    return s\n",
    "\n",
    "\n",
    "'''Funcao para calcular o erro médio da predição utilizando cross-entropy (Não pode ser utilizada com Relu, mas ok na output com softmax)\n",
    "    input:\n",
    "        Y: Labels\n",
    "        Y_pred: Labels Preditas\n",
    "    output:\n",
    "        custo: erro médio da predição\n",
    "'''\n",
    "def cross_entropy_loss(Y, Y_pred):\n",
    "    samples_number = Y.shape[1]\n",
    "    custo = -(1./samples_number) * ( np.sum( np.multiply(np.log(Y_pred),Y) ) + np.sum( np.multiply(np.log(1-Y_pred),(1-Y)) ) )\n",
    "    return custo\n",
    "\n",
    "\n",
    "'''Funcao para calcular o erro médio da predição utilizando cross-entropy multiclasse (Não pode ser utilizada com Relu, mas ok na output com softmax)\n",
    "    input:\n",
    "        Y: Labels\n",
    "        Y_pred: Labels Preditas\n",
    "    output:\n",
    "        custo: erro médio da predição\n",
    "'''\n",
    "def multiclass_cross_entropy_loss(Y, Y_pred):\n",
    "\n",
    "    aux = np.sum(np.multiply(Y, np.log(Y_pred)))\n",
    "    samples_number = Y.shape[1]\n",
    "    custo = -(1/samples_number) * aux\n",
    "\n",
    "    return custo\n",
    "    \n",
    "\n",
    "'''Funcao para calcular a probabilidade de ser de cada classe\n",
    "    input:\n",
    "        z: entrada da camada de saída\n",
    "    output:\n",
    "        p: probabilidade de ser de cada classe\n",
    "'''\n",
    "def softmax(z):\n",
    "    return np.exp(z) / np.sum(np.exp(z), axis=0)\n",
    "\n",
    "\n",
    "'''Funcao para calcular a matriz de confusão\n",
    "    input:\n",
    "        Y: labels corretas\n",
    "        Y_pred: labels preditas\n",
    "        classes: numéro de classes\n",
    "    output:\n",
    "        confusion_matrix: matriz de confusão\n",
    "'''\n",
    "def get_confusion_matrix(Y, Y_pred, classes):\n",
    "    \n",
    "    if(Y_pred.max() != 1):\n",
    "        Y_pred = binarize_labels(Y_pred)\n",
    "    \n",
    "    confusion_matrix = np.zeros((classes,classes)).astype(int)\n",
    "    \n",
    "    for y in range (Y.shape[1]):\n",
    "        for c_pred in range (classes):\n",
    "            if(Y_pred[c_pred, y] == 1):\n",
    "                for c in range (classes):\n",
    "                    if(Y[c, y] == 1):\n",
    "                        confusion_matrix[c, c_pred]+=1\n",
    "    \n",
    "    return confusion_matrix\n",
    "\n",
    "'''Funcao para converter um vetor de probabilidades de classes em one-hot-encoding\n",
    "    input:\n",
    "        L: vetor de probabilidade das labels\n",
    "    output:\n",
    "        Y_pred: vetor de labels (one-hot-encoding) \n",
    "    Ex: \n",
    "        Y_pred[:,1] = [0,1,0,0,0,0,0,0,0,0] quer dizer que a classe da sample 1 é 2\n",
    "'''\n",
    "def binarize_labels(L):\n",
    "    samples_number = L.shape[1]\n",
    "    classes = L.shape[0]\n",
    "    \n",
    "    Y_pred = np.argmax(L, axis=0).reshape(1, samples_number)\n",
    "\n",
    "    y_aux = np.eye(classes)[Y_pred.astype('int32')]\n",
    "    y_aux = y_aux.T.reshape(classes, samples_number)\n",
    "\n",
    "    Y_pred = y_aux\n",
    "    \n",
    "    return Y_pred\n",
    "\n",
    "'''Funcao para calcular a predição (Feed forward) de um dataframe dados parâmetros da rede neural com 1 camada escondida\n",
    "    input:\n",
    "        weights_1: vetor de pesos para calculo da entrada da camada escondida\n",
    "        weights_2: vetor de pesos para calculo da entrada da camada de saída\n",
    "        b1: vetor de pesos do bias para calculo da entrada da camada escondida\n",
    "        b2: vetor de pesos do bias para calculo da entrada da camada de saída\n",
    "        df: Dataframe a ser predito\n",
    "        activation_function: qual função de ativação será usada nos neurônios (relu, sigmoid, tanh)\n",
    "    output:\n",
    "        ol_output: vetor contendo as labels preditas\n",
    "'''\n",
    "def predict_labels_1_hidden(weights_1, weights_2, b1, b2, df, activation_function):\n",
    "    \n",
    "    hl_input = np.matmul(weights_1,df.T) + b1\n",
    "    hl_output = activation_function(hl_input)\n",
    "    ol_input = np.matmul(weights_2,hl_output) + b2\n",
    "    ol_output = softmax(ol_input)\n",
    "    \n",
    "    return ol_output\n",
    "\n",
    "'''Funcao para calcular a predição (Feed forward) de um dataframe dados parâmetros de uma rede neural com 2 camadas escondidas\n",
    "    input:\n",
    "        weights_1: vetor de pesos para calculo da entrada da primeira camada escondida\n",
    "        weights_2: vetor de pesos para calculo da entrada da segunda camada escondida\n",
    "        weights_2: vetor de pesos para calculo da entrada da camada de saída\n",
    "        b1: vetor de pesos do bias para calculo da entrada da primeira camada escondida\n",
    "        b1: vetor de pesos do bias para calculo da entrada da segunda camada escondida\n",
    "        b2: vetor de pesos do bias para calculo da entrada da camada de saída\n",
    "        df: Dataframe a ser predito\n",
    "        activation_function: qual função de ativação será usada nos neurônios (relu, sigmoid, tanh)\n",
    "    output:\n",
    "        ol_output: vetor contendo as labels preditas\n",
    "'''\n",
    "def predict_labels_2_hidden(weights_1, weights_2, weights_3, b1, b2, b3, df, activation_function):\n",
    "    \n",
    "    hl_1_input = np.matmul(weights_1,normalized_validation_df.T) + b1\n",
    "    hl_1_output = activation_function(hl_1_input) \n",
    "    hl_2_input = np.matmul(weights_2,hl_1_output) + b2\n",
    "    hl_2_output = activation_function(hl_2_input)\n",
    "    ol_input = np.matmul(weights_3,hl_2_output) + b3\n",
    "    ol_output = softmax(ol_input)\n",
    "    \n",
    "    return ol_output\n",
    "\n",
    "'''Funcao para obter as métricas de performance: precision, recall e f1_score\n",
    "    input:\n",
    "        confusion_matrix: matriz de confusão (use get_confusion_matrix() para calculá-la)\n",
    "    output:\n",
    "        precision: relação entre a quantidade de positivos preditos pela quantidade real de positivos \n",
    "        recall: relação entre a quantidade de positivos esperados pela quantidade de positivos preditos\n",
    "        f1_score: metrica para relacionar precision e recall em uma única métrica\n",
    "'''\n",
    "def get_metrics(confusion_matrix):\n",
    "    precision = get_precision(confusion_matrix)\n",
    "    recall = get_recall(confusion_matrix)\n",
    "    f1_score = get_f1_score(precision, recall)\n",
    "    \n",
    "    return precision, recall, f1_score\n",
    "\n",
    "def print_metrics(precision, recall, f1_score):\n",
    "    precision = np.around(precision, decimals=2).reshape( precision.shape[0],1)\n",
    "    recall = np.around(recall, decimals=2).reshape( recall.shape[0],1)\n",
    "    f1_score = np.around(f1_score, decimals=2).reshape( f1_score.shape[0],1)\n",
    "    \n",
    "    average_precision = np.sum(precision)/precision.shape[0]\n",
    "    average_recall = np.sum(recall)/recall.shape[0]\n",
    "    average_f1_score = np.sum(f1_score)/f1_score.shape[0]\n",
    "    \n",
    "    print(\"\\n\\nPrecision (Pr), Recall (Re) and F1_Score (F1) of each class: \")\n",
    "    print(\"Pr   Re    F1\")\n",
    "    print(re.sub(r' *\\n *', '\\n', np.array_str(np.c_[precision, recall, f1_score]).replace('[', '').replace(']', '').strip()))\n",
    "    \n",
    "    print(\"\\n\\nAverage Precision: \", round(average_precision,2) , \"\\nAverage Recall: \", round(average_recall, 2) ,\"\\nAverage F1_Score: \", round(average_f1_score,2))\n",
    "    \n",
    "    #print(precision,\" \", recall,\" \", f1_score)\n",
    "\n",
    "def get_recall(confusion_matrix):\n",
    "    precision = np.ones((confusion_matrix.shape[0]))\n",
    "    for i in range (confusion_matrix.shape[0]):\n",
    "        tp = confusion_matrix[i,i]\n",
    "        tp_fp = np.sum(confusion_matrix[:,i])\n",
    "        \n",
    "        precision[i] = tp/tp_fp\n",
    "        \n",
    "    return precision\n",
    "\n",
    "def get_precision(confusion_matrix):\n",
    "    recall = np.ones(confusion_matrix.shape[0])\n",
    "    \n",
    "    for i in range (confusion_matrix.shape[0]):\n",
    "        tp = confusion_matrix[i,i]\n",
    "        tp_fn = np.sum(confusion_matrix[i,:])\n",
    "        \n",
    "        recall[i] = tp/tp_fn\n",
    "        \n",
    "    return recall\n",
    "\n",
    "def get_f1_score(precision, recall):\n",
    "    f1 = 2*( (precision*recall) / (precision+recall) )\n",
    "    return f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparando os Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Dataframe......\n",
      "Separating into training and validation.....\n",
      "Normalizing (This may take a couple minutes).....\n",
      "Preparing the labels.....\n",
      "All done!.....\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading Dataframe......\")\n",
    "df = pd.read_csv(\"data/fashion-mnist_train.csv\")\n",
    "\n",
    "print(\"Separating into training and validation.....\")\n",
    "#Separate the Training DF into Train and Validation\n",
    "msk = np.random.rand(len(df)) < 0.7 \n",
    "\n",
    "train_df = df[msk]\n",
    "validation_df = df[~msk]\n",
    "\n",
    "Y_train = train_df[\"label\"]\n",
    "Y_validation = validation_df[\"label\"]\n",
    "\n",
    "train_df = train_df.loc[:, train_df.columns != \"label\"]\n",
    "validation_df = validation_df.loc[:, validation_df.columns != \"label\"]\n",
    "\n",
    "#test_df = pd.read_csv(\"data/fashion-mnist_test.csv\")\n",
    "\n",
    "print(\"Normalizing (This may take a couple minutes).....\")\n",
    "normalized_train_df = normalize_dataframe(train_df)\n",
    "normalized_validation_df = normalize_dataframe(validation_df)\n",
    "\n",
    "\n",
    "print(\"Preparing the labels.....\")\n",
    "#One hot encoding labels para o softmax\n",
    "classes = 10\n",
    "\n",
    "samples_train = Y_train.shape[0]\n",
    "samples_validation = Y_validation.shape[0]\n",
    "\n",
    "Y_train = np.array(Y_train).reshape(1, samples_train)\n",
    "\n",
    "y_aux = np.eye(classes)[Y_train.astype('int32')]\n",
    "y_aux = y_aux.T.reshape(classes, samples_train)\n",
    "Y_train = y_aux\n",
    "\n",
    "Y_validation = np.array(Y_validation).reshape(1, samples_validation)\n",
    "\n",
    "y_aux = np.eye(classes)[Y_validation.astype('int32')]\n",
    "y_aux = y_aux.T.reshape(classes, samples_validation)\n",
    "\n",
    "Y_validation = y_aux\n",
    "print(\"All done!.....\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualização dos Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "i = 3\n",
    "#print(np.array(train_df.iloc[1]).reshape(28,28))\n",
    "plt.imshow(np.array(train_df.iloc[5]).reshape(28,28), cmap = matplotlib.cm.binary)\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "print(Y_train[:,5])\n",
    "\n",
    "##Classes:\n",
    "# 0 \tT-shirt/top\n",
    "# 1 \tTrouser\n",
    "# 2 \tPullover\n",
    "# 3 \tDress\n",
    "# 4 \tCoat\n",
    "# 5 \tSandal\n",
    "# 6 \tShirt\n",
    "# 7 \tSneaker\n",
    "# 8 \tBag\n",
    "# 9 \tAnkle boot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rede Neural com 1 Camada Escondida (Multi Class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 cost:  2.302254437312288\n",
      "Epoch 100 cost:  0.7596372945334771\n",
      "Epoch 200 cost:  0.5606936667361904\n",
      "Epoch 300 cost:  0.5152609067253606\n",
      "Epoch 400 cost:  0.4731752880655705\n",
      "Epoch 500 cost:  0.4453109912710704\n",
      "Epoch 600 cost:  0.42749030008447175\n",
      "Epoch 700 cost:  0.4136468023202372\n",
      "Epoch 800 cost:  0.4020598930089031\n",
      "Epoch 900 cost:  0.38775448239730653\n",
      "Epoch 1000 cost:  0.37427946860137146\n",
      "Epoch 1100 cost:  0.3720391647969387\n",
      "Epoch 1200 cost:  0.36558956106522467\n",
      "Epoch 1300 cost:  0.36195124415921137\n",
      "Epoch 1400 cost:  0.3480141578314059\n",
      "Epoch 1500 cost:  0.3407822769338691\n",
      "Epoch 1600 cost:  0.34455482848428504\n",
      "Epoch 1700 cost:  0.330711161787308\n",
      "Epoch 1800 cost:  0.33019638182425615\n",
      "Epoch 1900 cost:  0.32537152119232327\n",
      "Epoch 2000 cost:  0.3164805907404057\n",
      "Epoch 2100 cost:  0.31031514349467626\n",
      "Epoch 2200 cost:  0.3087669599256484\n",
      "Epoch 2300 cost:  0.30887276554264803\n",
      "Epoch 2400 cost:  0.3270505687553648\n",
      "Epoch 2500 cost:  0.30253962207419494\n",
      "Epoch 2600 cost:  0.2945926508538348\n",
      "Epoch 2700 cost:  0.2983613649897668\n",
      "Epoch 2800 cost:  0.2953114971701471\n",
      "Epoch 2900 cost:  0.2866406054727389\n",
      "Final cost: 0.28391789372737825\n"
     ]
    }
   ],
   "source": [
    "#ONE HIDDEN LAYER\n",
    "\n",
    "classes = 10\n",
    "n_hl = 32\n",
    "learning_rate = 0.3\n",
    "\n",
    "# Can be relu, sigmoid or tanh\n",
    "activation_function = relu\n",
    "#This is the derivative of the activation function denoted by <name>_\n",
    "activation_function_ = relu_\n",
    "\n",
    "iterations = 3000\n",
    "\n",
    "df = train_df.T\n",
    "Y = Y_train\n",
    "\n",
    "n_x = df.shape[0]\n",
    "m = df.shape[1]\n",
    "\n",
    "weights_1 = np.random.randn(n_hl, n_x) * 0.01\n",
    "b1 = np.zeros((n_hl, 1))\n",
    "weights_2 = np.random.randn(classes, n_hl) *0.01\n",
    "b2 = np.zeros((classes, 1))\n",
    "\n",
    "start_time = time.time()\n",
    "for i in range(iterations):\n",
    "\n",
    "    #Feed forward\n",
    "    hl_input = np.matmul(weights_1,df) + b1\n",
    "    hl_output = activation_function(hl_input)\n",
    "    ol_input = np.matmul(weights_2,hl_output) + b2\n",
    "    ol_output = softmax(ol_input)\n",
    "\n",
    "    #Calculate the error\n",
    "    cost = multiclass_cross_entropy_loss(Y, ol_output)\n",
    "\n",
    "    #Backpropagation\n",
    "    d_ol_input = ol_output-Y\n",
    "    d_weights_2 = (1./m) * np.matmul(d_ol_input, hl_output.T)\n",
    "    d_b2 = (1./m) * np.sum(d_ol_input, axis=1, keepdims=True)\n",
    "\n",
    "    d_hl_output = np.matmul(weights_2.T, d_ol_input)\n",
    "    d_hl_input = d_hl_output * activation_function_(hl_input)\n",
    "    d_weights_1 = (1./m) * np.matmul(d_hl_input, df.T)\n",
    "    d_b1 = (1./m) * np.sum(d_hl_input, axis=1, keepdims=True)\n",
    "\n",
    "    #Atualização dos pesos e biases\n",
    "    weights_2 = weights_2 - learning_rate * d_weights_2\n",
    "    b2 = b2 - learning_rate * d_b2\n",
    "    weights_1 = weights_1 - learning_rate * d_weights_1\n",
    "    b1 = b1 - learning_rate * d_b1\n",
    "\n",
    "    if (i % 100 == 0):\n",
    "        print(\"Epoch\", i, \"cost: \", cost)\n",
    "\n",
    "final_time = time.time() - start_time\n",
    "print(\"Final cost:\", cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1579    6   16   30   11    0  182    0   16    0]\n",
      " [  12 1756    2   35    6    0    4    0    0    0]\n",
      " [  34    4 1302   12  308    0  159    0   13    0]\n",
      " [  90   26   13 1492   93    0   57    0    4    0]\n",
      " [   8    3   76   42 1569    0  109    0    6    0]\n",
      " [   1    0    0    2    0 1720    0   49    9   30]\n",
      " [ 246    7  104   35  183    1 1221    0   19    0]\n",
      " [   0    0    0    0    0   49    0 1631    7   74]\n",
      " [   6    0    4    7   12   11   21    6 1728    1]\n",
      " [   0    0    1    0    0   25    0   41    3 1776]]\n",
      "\n",
      "\n",
      "Precision (Pr), Recall (Re) and F1_Score (F1) of each class: \n",
      "Pr   Re    F1\n",
      "0.86 0.8  0.83\n",
      "0.97 0.97 0.97\n",
      "0.71 0.86 0.78\n",
      "0.84 0.9  0.87\n",
      "0.87 0.72 0.79\n",
      "0.95 0.95 0.95\n",
      "0.67 0.7  0.68\n",
      "0.93 0.94 0.94\n",
      "0.96 0.96 0.96\n",
      "0.96 0.94 0.95\n",
      "\n",
      "\n",
      "Average Precision:  0.87 \n",
      "Average Recall:  0.87 \n",
      "Average F1_Score:  0.87\n"
     ]
    }
   ],
   "source": [
    "prediction = predict_labels_1_hidden(weights_1, weights_2, b1, b2, normalized_validation_df, activation_function)\n",
    "\n",
    "samples_number = prediction.shape[1]\n",
    "\n",
    "predictions = np.argmax(prediction, axis=0)\n",
    "labels = np.argmax(Y_validation, axis=0)\n",
    "\n",
    "confusion_matrix = get_confusion_matrix(Y_validation, prediction, classes)\n",
    "\n",
    "print(confusion_matrix)\n",
    "\n",
    "precision, recall, f1_score = get_metrics(confusion_matrix)\n",
    "\n",
    "print_metrics(precision, recall, f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tempo gasto:  304.97782254219055\n"
     ]
    }
   ],
   "source": [
    "print(\"Tempo gasto: \", final_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rede Neural com 2 Camadas Escondidas (Multiclasse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 cost:  2.3024974971809304\n",
      "Epoch 100 cost:  1.3595747828070868\n",
      "Epoch 200 cost:  0.8619044208143345\n",
      "Epoch 300 cost:  0.6634284236529474\n",
      "Epoch 400 cost:  0.5882340048606266\n",
      "Epoch 500 cost:  0.5474145661036904\n",
      "Epoch 600 cost:  0.49747964394350097\n",
      "Epoch 700 cost:  0.4812147011861902\n",
      "Epoch 800 cost:  0.43495618673421566\n",
      "Epoch 900 cost:  0.4369450464230194\n",
      "Epoch 1000 cost:  0.42203620091970356\n",
      "Epoch 1100 cost:  0.3855807499184543\n",
      "Epoch 1200 cost:  0.4039447302378841\n",
      "Epoch 1300 cost:  0.3844083332328202\n",
      "Epoch 1400 cost:  0.3601098982372285\n",
      "Epoch 1500 cost:  0.3634844080047194\n",
      "Epoch 1600 cost:  0.34345831971705154\n",
      "Epoch 1700 cost:  0.36436293840247774\n",
      "Epoch 1800 cost:  0.33990781165092543\n",
      "Epoch 1900 cost:  0.39048055819377225\n",
      "Epoch 2000 cost:  0.3199028906073081\n",
      "Epoch 2100 cost:  0.32926579188159916\n",
      "Epoch 2200 cost:  0.3210403326074115\n",
      "Epoch 2300 cost:  0.3034225933016322\n",
      "Epoch 2400 cost:  0.29835388377993977\n",
      "Epoch 2500 cost:  0.3039252185533242\n",
      "Epoch 2600 cost:  0.2903535436782421\n",
      "Epoch 2700 cost:  0.282707782186637\n",
      "Epoch 2800 cost:  0.2885965464989461\n",
      "Epoch 2900 cost:  0.2807013508711309\n",
      "Final cost: 0.27489065183241357\n"
     ]
    }
   ],
   "source": [
    "#ONE HIDDEN LAYER\n",
    "\n",
    "classes = 10\n",
    "n_hl_1 = 32\n",
    "n_hl_2 = 32\n",
    "learning_rate = 0.3\n",
    "\n",
    "activation_function = tanh\n",
    "activation_function_ = tanh_\n",
    "\n",
    "iterations = 3000\n",
    "\n",
    "df = train_df.T\n",
    "Y = Y_train\n",
    "\n",
    "n_x = df.shape[0]\n",
    "m = df.shape[1]\n",
    "\n",
    "weights_1 = np.random.randn(n_hl_1, n_x) * 0.01\n",
    "b1 = np.zeros((n_hl_1, 1))\n",
    "weights_2 = np.random.randn(n_hl_2, n_hl_1) *0.01\n",
    "b2 = np.zeros((n_hl_2, 1))\n",
    "weights_3 = np.random.randn(classes, n_hl_2) *0.01\n",
    "b3 = np.zeros((classes, 1))\n",
    "\n",
    "start_time = time.time()\n",
    "for i in range(iterations):\n",
    "\n",
    "    #Feed forward\n",
    "    hl_1_input = np.matmul(weights_1,df) + b1\n",
    "    hl_1_output = activation_function(hl_1_input) \n",
    "    hl_2_input = np.matmul(weights_2,hl_1_output) + b2\n",
    "    hl_2_output = activation_function(hl_2_input)\n",
    "    ol_input = np.matmul(weights_3,hl_2_output) + b3\n",
    "    ol_output = softmax(ol_input)\n",
    "\n",
    "    #Calculate the error\n",
    "    cost = multiclass_cross_entropy_loss(Y, ol_output)\n",
    "\n",
    "    #Backpropagation\n",
    "    d_ol_input = ol_output-Y\n",
    "    d_weights_3 = (1./m) * np.matmul(d_ol_input, hl_2_output.T)\n",
    "    d_b3 = (1./m) * np.sum(d_ol_input, axis=1, keepdims=True)\n",
    "    \n",
    "    d_hl_2_output = np.matmul(weights_3.T, d_ol_input)\n",
    "    d_hl_2_input = d_hl_2_output * activation_function_(hl_2_input)\n",
    "    d_weights_2 = (1./m) * np.matmul(d_hl_2_input, hl_1_output.T)\n",
    "    d_b2 = (1./m) * np.sum(d_hl_2_input, axis=1, keepdims=True)\n",
    "\n",
    "    d_hl_1_output = np.matmul(weights_2.T, d_hl_2_input)\n",
    "    d_hl_1_input = d_hl_1_output * activation_function_(hl_1_input)\n",
    "    d_weights_1 = (1./m) * np.matmul(d_hl_1_input, df.T)\n",
    "    d_b1 = (1./m) * np.sum(d_hl_1_input, axis=1, keepdims=True)\n",
    "\n",
    "    #Atualização dos pesos e biases\n",
    "    weights_3 = weights_3 - learning_rate * d_weights_3\n",
    "    b3 = b3 - learning_rate * d_b3\n",
    "    weights_2 = weights_2 - learning_rate * d_weights_2\n",
    "    b2 = b2 - learning_rate * d_b2\n",
    "    weights_1 = weights_1 - learning_rate * d_weights_1\n",
    "    b1 = b1 - learning_rate * d_b1\n",
    "\n",
    "    if (i % 100 == 0):\n",
    "        print(\"Epoch\", i, \"cost: \", cost)\n",
    "    \n",
    "final_time = time.time() - start_time\n",
    "\n",
    "print(\"Final cost:\", cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shapes (10,32) and (10,18105) not aligned: 32 (dim 1) != 10 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-3305321b4582>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict_labels_2_hidden\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights_2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights_3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalized_validation_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation_function\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#Z1 = np.matmul(weights_1, normalized_validation_df.T) + b1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#A1 = relu(Z1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-a3e864a3fc2f>\u001b[0m in \u001b[0;36mpredict_labels_2_hidden\u001b[0;34m(weights_1, weights_2, weights_3, b1, b2, b3, df, activation_function)\u001b[0m\n\u001b[1;32m    207\u001b[0m     \u001b[0mhl_2_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights_2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mhl_1_output\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mb2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m     \u001b[0mhl_2_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mactivation_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhl_2_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 209\u001b[0;31m     \u001b[0mol_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights_3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mhl_2_output\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mb3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    210\u001b[0m     \u001b[0mol_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mol_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: shapes (10,32) and (10,18105) not aligned: 32 (dim 1) != 10 (dim 0)"
     ]
    }
   ],
   "source": [
    "prediction = predict_labels_2_hidden(weights_1, weights_2, weights_3, b1, b2, b3, normalized_validation_df, activation_function)\n",
    "\n",
    "\n",
    "#Z1 = np.matmul(weights_1, normalized_validation_df.T) + b1\n",
    "#A1 = relu(Z1)\n",
    "#Z2 = np.matmul(weights_2, A1) + b2\n",
    "#A2 = relu(Z2)\n",
    "#Z3 = np.matmul(weights_3, A2) + b3\n",
    "#A3 = softmax(Z3)\n",
    "\n",
    "predictions = np.argmax(prediction, axis=0)\n",
    "labels = np.argmax(Y_validation, axis=0)\n",
    "\n",
    "confusion_matrix = get_confusion_matrix(Y_validation, prediction, classes)\n",
    "\n",
    "print(confusion_matrix)\n",
    "\n",
    "precision, recall, f1_score = get_metrics(confusion_matrix)\n",
    "\n",
    "print_metrics(precision, recall, f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tempo gasto:  430.2525317668915\n"
     ]
    }
   ],
   "source": [
    "print(\"Tempo gasto: \", final_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
