{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.set_printoptions(threshold=np.nan)\n",
    "from sklearn.datasets import fetch_mldata\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "# changing labels to make one vs all\n",
    "def change_label(which_class,y_train,y_test):\n",
    "\n",
    "    samples_train = y_train.shape[0]\n",
    "    samples_test = y_test.shape[0]\n",
    "    \n",
    "    new_labels_train = np.zeros((samples_train,1))\n",
    "    new_labels_test = np.zeros((samples_test,1))\n",
    "\n",
    "    new_labels_train = np.array(y_train == which_class).astype(int)\n",
    "    new_labels_test = np.array(y_test == which_class).astype(int)\n",
    "    \n",
    "#     ind_row_tr, ind_col_tr = np.where(y_train == which_class)\n",
    "#     ind_row_tst, ind_col_tst = np.where(y_test== which_class)\n",
    "    \n",
    "#     new_labels_train[ind_row_tr,ind_col_tr] = 1\n",
    "#     new_labels_test[ind_row_tst,ind_col_tst] = 1\n",
    "    \n",
    "    return new_labels_train, new_labels_test\n",
    "    \n",
    "    \n",
    "'''Criando a funcao de normalização de um dataframe inteiro\n",
    "    input:\n",
    "        df: Dataframe\n",
    "    output:\n",
    "        df: Dataframe com valores normalizados\n",
    "'''\n",
    "def normalize_dataframe(df):\n",
    "    for column in df:\n",
    "        df[column] = df[column]/255\n",
    "    return df\n",
    "\n",
    "\n",
    "'''Funcao para obter as métricas de performance: precision, recall e f1_score\n",
    "    input:\n",
    "        confusion_matrix: matriz de confusão (use get_confusion_matrix() para calculá-la)\n",
    "    output:\n",
    "        precision: relação entre a quantidade de positivos preditos pela quantidade real de positivos \n",
    "        recall: relação entre a quantidade de positivos esperados pela quantidade de positivos preditos\n",
    "        f1_score: metrica para relacionar precision e recall em uma única métrica\n",
    "'''\n",
    "def get_metrics(confusion_matrix):\n",
    "    precision = get_precision(confusion_matrix)\n",
    "    recall = get_recall(confusion_matrix)\n",
    "    f1_score = get_f1_score(precision, recall)\n",
    "    \n",
    "    return precision, recall, f1_score\n",
    "\n",
    "def print_metrics(precision, recall, f1_score):\n",
    "    precision = np.around(precision, decimals=2).reshape( precision.shape[0],1)\n",
    "    recall = np.around(recall, decimals=2).reshape( recall.shape[0],1)\n",
    "    f1_score = np.around(f1_score, decimals=2).reshape( f1_score.shape[0],1)\n",
    "    \n",
    "    average_precision = np.sum(precision)/precision.shape[0]\n",
    "    average_recall = np.sum(recall)/recall.shape[0]\n",
    "    average_f1_score = np.sum(f1_score)/f1_score.shape[0]\n",
    "    \n",
    "    print(\"\\n\\nPrecision (Pr), Recall (Re) and F1_Score (F1) of each class: \")\n",
    "    print(\"Pr   Re    F1\")\n",
    "    print(re.sub(r' *\\n *', '\\n', np.array_str(np.c_[precision, recall, f1_score]).replace('[', '').replace(']', '').strip()))\n",
    "    \n",
    "    print(\"\\n\\nAverage Precision: \", round(average_precision,2) , \"\\nAverage Recall: \", round(average_recall, 2) ,\"\\nAverage F1_Score: \", round(average_f1_score,2))\n",
    "    \n",
    "    #print(precision,\" \", recall,\" \", f1_score)\n",
    "\n",
    "def get_recall(confusion_matrix):\n",
    "    precision = np.ones((confusion_matrix.shape[0]))\n",
    "    for i in range (confusion_matrix.shape[0]):\n",
    "        tp = confusion_matrix[i,i]\n",
    "        tp_fp = np.sum(confusion_matrix[:,i])\n",
    "        \n",
    "        precision[i] = tp/tp_fp\n",
    "        \n",
    "    return precision\n",
    "\n",
    "def get_precision(confusion_matrix):\n",
    "    recall = np.ones(confusion_matrix.shape[0])\n",
    "    \n",
    "    for i in range (confusion_matrix.shape[0]):\n",
    "        tp = confusion_matrix[i,i]\n",
    "        tp_fn = np.sum(confusion_matrix[i,:])\n",
    "        \n",
    "        recall[i] = tp/tp_fn\n",
    "        \n",
    "    return recall\n",
    "\n",
    "def get_f1_score(precision, recall):\n",
    "    f1 = 2*( (precision*recall) / (precision+recall) )\n",
    "    return f1\n",
    "\n",
    "'''Funcao para calcular a matriz de confusão\n",
    "    input:\n",
    "        Y: labels corretas\n",
    "        Y_pred: labels preditas\n",
    "        classes: numéro de classes\n",
    "    output:\n",
    "        confusion_matrix: matriz de confusão\n",
    "'''\n",
    "def get_confusion_matrix(Y, Y_pred, classes):\n",
    "    \n",
    "    \n",
    "    \n",
    "    if(Y_pred.max() != 1):\n",
    "        Y_pred = binarize_labels(Y_pred,classes)\n",
    "        \n",
    "    if(Y.max() != 1 ):\n",
    "        Y = binarize_labels(Y,classes)\n",
    "    \n",
    "    confusion_matrix = np.zeros((classes,classes)).astype(int)\n",
    "    \n",
    "    for y in range (Y.shape[1]):\n",
    "        for c_pred in range (classes):\n",
    "            if(Y_pred[c_pred, y] == 1):\n",
    "                for c in range (classes):\n",
    "                    if(Y[c, y] == 1):\n",
    "                        confusion_matrix[c, c_pred]+=1\n",
    "    \n",
    "    return confusion_matrix\n",
    "\n",
    "\n",
    "'''Funcao para converter um vetor de probabilidades de classes em one-hot-encoding\n",
    "    input:\n",
    "        L: vetor de probabilidade das labels\n",
    "    output:\n",
    "        Y_pred: vetor de labels (one-hot-encoding) \n",
    "    Ex: \n",
    "        Y_pred[:,1] = [0,1,0,0,0,0,0,0,0,0] quer dizer que a classe da sample 1 é 2\n",
    "'''\n",
    "def binarize_labels(L, classes):\n",
    "    samples_number = L.shape[0]\n",
    "    #classes = L.shape[0]\n",
    "    \n",
    "    Y_pred = np.array(L).reshape(1, samples_number)\n",
    "\n",
    "    y_aux = np.eye(classes)[Y_pred.astype('int32')]\n",
    "    y_aux = y_aux.T.reshape(classes, samples_number)\n",
    "\n",
    "    Y_pred = y_aux\n",
    "    \n",
    "    return Y_pred\n",
    "\n",
    "# plot image \n",
    "def plot_image(this_digit):\n",
    "    some_digit_image = this_digit.reshape(28,28)\n",
    "    plt.imshow(some_digit_image, cmap = matplotlib.cm.binary, interpolation = \"nearest\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "#create sigmoid function\n",
    "def sigmoid(z):\n",
    "    s = 1.0 / (1.0 + np.exp(-z))\n",
    "    return s\n",
    "\n",
    "\n",
    "#initialize values of thetas and bias\n",
    "def initialize_parameters(dim):\n",
    "    w = np.zeros((dim,1))\n",
    "    b = 0\n",
    "    return w,b\n",
    "\n",
    "#implement cost function and its gradient propagation\n",
    "def cost_function(w, b, X, Y):\n",
    "\n",
    "    m = X.shape[1] #quantity of samples\n",
    "    \n",
    "    z = np.dot(w.T,X)+b #hipotese without sigmoid\n",
    "    A = sigmoid(z) # applying hipotese on sigmoid, A is the result of sigmoid for each sample\n",
    "    cost = -1.0/m*np.sum(Y*np.log(A)+(1.0-Y)*np.log(1.0-A)) #find cost values for all samples\n",
    "    dw = 1.0/m*np.dot(X, (A-Y).T) #find residual of weights\n",
    "    db = 1.0/m*np.sum(A-Y) # find residual of bias\n",
    "    cost = np.squeeze(cost)\n",
    "    \n",
    "    return dw,db, cost\n",
    "\n",
    "\n",
    "#apply gradient descent\n",
    "def gradient_descent(w,b,X,Y,num_iterations,alpha, print_cost):\n",
    "    cost_array = []\n",
    "    for i in range(num_iterations):\n",
    "        #call cost function\n",
    "        dw,db,cost = cost_function(w,b,X,Y)\n",
    "        # update weights and bias\n",
    "        w = w - alpha*dw\n",
    "        b = b - alpha*db\n",
    "        \n",
    "        #save cost\n",
    "        cost_array.append(cost)\n",
    "        \n",
    "        if (print_cost == True and i%100 == 0):\n",
    "            print(\"Cost from iteration \",i,\"= \",cost,\"\\n\")\n",
    "            \n",
    "    return w,b,cost_array\n",
    "\n",
    "\n",
    "#prediction\n",
    "def prediction (w,b,X,threshold):\n",
    "    samples = X.shape[1]\n",
    "    Y_prediction = np.zeros((1,samples))\n",
    "    w = w.reshape(X.shape[0],1)\n",
    "    A = sigmoid (np.dot(w.T, X)+b)\n",
    "    return A\n",
    "\n",
    "# one vs all prediction\n",
    "def one_vs_all(w,X):\n",
    "    samples = X.shape[1]\n",
    "    weighs_qut = w.shape[0]\n",
    "    bias = w[-1]\n",
    "    weights = w[:(len(w))-1]\n",
    "    A = sigmoid (np.dot(weights.T, X)+bias)\n",
    "    \n",
    "    return A;\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# #Data preparation\n",
    "\n",
    "# def data_preparation(X,Y):\n",
    "    \n",
    "#     X_binary = X[np.any([y == 1,y == 2, y==3], axis = 0)]\n",
    "#     y_binary = y[np.any([y == 1,y == 2, y==3], axis = 0)]\n",
    "#     shuffle_index = np.random.permutation(X_binary.shape[0])\n",
    "#     X_binary_shuffled, y_binary_shuffled = X_binary[shuffle_index], y_binary[shuffle_index]\n",
    "    \n",
    "#     train_proportion = 0.8\n",
    "#     train_test_cut = int(len(X_binary)*train_proportion)\n",
    "    \n",
    "#     #sufle create new data set\n",
    "#     X_train, X_test, y_train, y_test = \\\n",
    "#     X_binary_shuffled[:train_test_cut], \\\n",
    "#     X_binary_shuffled[train_test_cut:], \\\n",
    "#     y_binary_shuffled[:train_test_cut], \\\n",
    "#     y_binary_shuffled[train_test_cut:]\n",
    "    \n",
    "#     #normalize\n",
    "#     X_train_normalised= X_train/255.0\n",
    "#     X_test_normalised = X_test/255.0\n",
    "    \n",
    "#     #reshape -> feature in rows and label in collumn\n",
    "#     X_train_tr = X_train_normalised.transpose()\n",
    "#     y_train_tr = y_train.reshape(1,y_train.shape[0])\n",
    "#     X_test_tr = X_test_normalised.transpose()\n",
    "#     y_test_tr = y_test.reshape(1,y_test.shape[0])\n",
    "    \n",
    "#     #change labels from 1-2 to 0-1\n",
    "# #     y_train_shifted = y_train_tr - 1\n",
    "# #     print(y_binary.shape)\n",
    "# #     y_test_shifted = y_test_tr - 1\n",
    "# #     print(\"Shape of X_train is\", X_train.shape)\n",
    "# #     print(\"Shape of X_test is\", X_test.shape)\n",
    "# #     print(\"Shape of y_train is\", y_train.shape)\n",
    "# #     print(\"Shape of y_test is\", y_test.shape)\n",
    "    \n",
    "# #     print(X_train_tr.shape)\n",
    "# #     print(y_train_tr.shape)\n",
    "# #     print(X_test_tr.shape)\n",
    "# #     print(y_test_tr.shape)\n",
    "\n",
    "    \n",
    "#     return (X_train_tr,X_test_tr,y_train_tr,y_test_tr)\n",
    "#     #for i in range(10):\n",
    "#         #print (\"digit\", i, \"appear\", np.count_nonzero(y == i), \"times\")\n",
    "       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Dataframe......\n",
      "Separating into training and validation.....\n",
      "Normalizing.....\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading Dataframe......\")\n",
    "df = pd.read_csv(\"data/fashion-mnist_train.csv\")\n",
    "\n",
    "#Separate the Training DF into Train and Validation\n",
    "msk = np.random.rand(len(df)) < 0.7 \n",
    "\n",
    "print(\"Separating into training and validation.....\")\n",
    "train_df = df[msk]\n",
    "validation_df = df[~msk]\n",
    "\n",
    "Y_train = train_df[\"label\"]\n",
    "Y_validation = validation_df[\"label\"]\n",
    "\n",
    "train_df = train_df.loc[:, train_df.columns != \"label\"]\n",
    "validation_df = validation_df.loc[:, validation_df.columns != \"label\"]\n",
    "\n",
    "#test_df = pd.read_csv(\"data/fashion-mnist_test.csv\")\n",
    "\n",
    "print(\"Normalizing (This may take a couple minutes).....\")\n",
    "normalized_train_df = normalize_dataframe(train_df)\n",
    "normalized_validation_df = normalize_dataframe(validation_df)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainig class  0\n",
      "Trainig class  1\n",
      "Trainig class  2\n",
      "Trainig class  3\n",
      "Trainig class  4\n",
      "Trainig class  5\n",
      "Trainig class  6\n",
      "Trainig class  7\n",
      "Trainig class  8\n",
      "Trainig class  9\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "X_train = normalized_train_df.T\n",
    "X_test = normalized_validation_df.T\n",
    "\n",
    "\n",
    "i = 0;\n",
    "dim_train = X_train.shape[0]\n",
    "dim_test = X_test.shape[0]\n",
    "num_classes = 10\n",
    "num_iterations = 1000\n",
    "alpha = 0.3\n",
    "\n",
    "y_train = Y_train\n",
    "y_test = Y_validation\n",
    "\n",
    "classifiers = {} #dicionarios de classificadores treinados\n",
    "#probabilities = np.zeros((num_classes,dim_test))\n",
    "probabilities = []\n",
    "while i < num_classes:\n",
    "    print(\"Trainig class \", i)\n",
    "    new_label_train, new_label_test = change_label(i,y_train,y_test)\n",
    "    ws, bs = initialize_parameters(dim_train)\n",
    "    \n",
    "    #Mude o change cost para True para verbose dos custos\n",
    "    w,b,costs = gradient_descent(ws,bs,X_train,new_label_train,num_iterations,alpha,print_cost=False) # pesos e bias treinados\n",
    "    treined_parameters = (np.append(w,b)).reshape(dim_train+1,1)  # concatenate weights\n",
    "    classifiers.update({i:treined_parameters})\n",
    "    i = i+1\n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1431   15   12   81   12    4  268    0   29    1]\n",
      " [   7 1688    9   51    9    0   14    0    2    0]\n",
      " [  16    7 1047   14  334    1  301    0   17    0]\n",
      " [  67   32    3 1592   83    0   94    0    4    0]\n",
      " [   3    1   62   40 1448    0  239    0    9    0]\n",
      " [   1    0    1    1    0 1610    3  104   11   80]\n",
      " [ 199    4  110   44  179    0 1188    0   42    1]\n",
      " [   0    0    0    0    0   55    0 1660    6  104]\n",
      " [   1    1    2   23    8   11   50    9 1643    1]\n",
      " [   1    0    0    0    0   29    1   72    1 1683]]\n",
      "\n",
      "\n",
      "Precision (Pr), Recall (Re) and F1_Score (F1) of each class: \n",
      "Pr   Re    F1\n",
      "0.77 0.83 0.8\n",
      "0.95 0.97 0.96\n",
      "0.6  0.84 0.7\n",
      "0.85 0.86 0.86\n",
      "0.8  0.7  0.75\n",
      "0.89 0.94 0.91\n",
      "0.67 0.55 0.61\n",
      "0.91 0.9  0.9\n",
      "0.94 0.93 0.94\n",
      "0.94 0.9  0.92\n",
      "\n",
      "\n",
      "Average Precision:  0.83 \n",
      "Average Recall:  0.84 \n",
      "Average F1_Score:  0.84\n"
     ]
    }
   ],
   "source": [
    "j=0\n",
    "\n",
    "probabilities = []\n",
    "#compute probabilities for each class\n",
    "while j < num_classes:\n",
    "    prob = one_vs_all(classifiers.get(j),X_test)\n",
    "    #probabilities[:][:j] = prob\n",
    "    #print(prob.shape)\n",
    "    probabilities.append(prob)\n",
    "    #print(np.array(probabilities).shape)\n",
    "    j = j+1\n",
    "\n",
    "k = 0\n",
    "aux_array = np.zeros(num_classes)\n",
    "aux_array_2 = np.zeros((num_classes, X_test.shape[1]))\n",
    "#print(np.array(aux_array_2).shape)\n",
    "prediction_array = []\n",
    "y = y_test\n",
    "\n",
    "for c in range (num_classes):\n",
    "    aux_array_2[c] = probabilities[c][0]\n",
    "\n",
    "final = np.array(np.argmax(aux_array_2, axis=0))\n",
    "\n",
    "y_test_new = y_test.T\n",
    "#print(\"oi\", final.shape)\n",
    "\n",
    "confusion_matrix = get_confusion_matrix(y_test_new,final, 10)\n",
    "\n",
    "print(confusion_matrix)\n",
    "\n",
    "precision, recall, f1_score = get_metrics(confusion_matrix)\n",
    "\n",
    "print_metrics(precision, recall, f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
