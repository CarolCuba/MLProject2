{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usando o Modelo Selecionado no conjunto de teste"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funções das Redes Neurais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "'''Criando a funcao de normalização de um dataframe inteiro\n",
    "    input:\n",
    "        df: Dataframe\n",
    "    output:\n",
    "        df: Dataframe com valores normalizados\n",
    "'''\n",
    "def normalize_dataframe(df):\n",
    "    for column in df:\n",
    "        df[column] = df[column]/255\n",
    "    return df\n",
    "\n",
    "\n",
    "'''Funcao para aplicar relu em um vetor\n",
    "    input:\n",
    "        z: weights * params + bias\n",
    "    output:\n",
    "        s: 0 para valores negativos z para valores positivos aplicado em cada amostra\n",
    "'''    \n",
    "def relu(z):\n",
    "    return np.maximum(z, 0)\n",
    "\n",
    "\n",
    "'''Funcao para aplicar a derivada da relu em um vetor\n",
    "    input:\n",
    "        z: weights * params + bias\n",
    "    output:\n",
    "        s: 0 para valores negativos 1 para valores positivos aplicado em cada amostra\n",
    "'''    \n",
    "def relu_(z):\n",
    "    return np.array(z>0).astype(int)\n",
    "\n",
    "\n",
    "'''Funcao para aplicar sigmoid em um vetor\n",
    "    input:\n",
    "        z: weights * params + bias\n",
    "    output:\n",
    "        s: valor entre 0-1 da aplicação da função para cada amostra\n",
    "'''\n",
    "def sigmoid(z):\n",
    "    #print(z.shape)\n",
    "    s = 1 / (1 + np.exp(-z))\n",
    "    return s\n",
    "\n",
    "\n",
    "'''Funcao para aplicar a derivada da sigmoid em um vetor\n",
    "    input:\n",
    "        z: weights * params + bias\n",
    "    output:\n",
    "        p: taxa de variação da sigmoid para cada amostra\n",
    "'''\n",
    "def sigmoid_(z):\n",
    "    #print(sigmoid(Z1).shape)\n",
    "    s = sigmoid(z) * (1 - sigmoid(z))\n",
    "    return s\n",
    "\n",
    "\n",
    "'''Funcao para calcular a tangente hiporbólica em um vetor\n",
    "    input:\n",
    "        x: weights * params + bias\n",
    "    output:\n",
    "        s: valor entre 0-1 da aplicação da função para cada amostra\n",
    "'''\n",
    "def tanh(x):\n",
    "    s = (2 / (1 + np.exp(-2*x))) -1\n",
    "    return s\n",
    "\n",
    "\n",
    "'''Funcao para calcular a derivada da tangente hiporbólica em um vetor\n",
    "    input:\n",
    "        x: weights * params + bias\n",
    "    output:\n",
    "        s: porcentagem de chance do valor ser da classe predita para cada amostra\n",
    "'''\n",
    "def tanh_(x):\n",
    "    s = 1 - tanh(x)**2\n",
    "    return s\n",
    "\n",
    "\n",
    "'''Funcao para calcular o erro médio da predição utilizando cross-entropy (Não pode ser utilizada com Relu, mas ok na output com softmax)\n",
    "    input:\n",
    "        Y: Labels\n",
    "        Y_pred: Labels Preditas\n",
    "    output:\n",
    "        custo: erro médio da predição\n",
    "'''\n",
    "def cross_entropy_loss(Y, Y_pred):\n",
    "    samples_number = Y.shape[1]\n",
    "    custo = -(1./samples_number) * ( np.sum( np.multiply(np.log(Y_pred),Y) ) + np.sum( np.multiply(np.log(1-Y_pred),(1-Y)) ) )\n",
    "    return custo\n",
    "\n",
    "\n",
    "'''Funcao para calcular o erro médio da predição utilizando cross-entropy multiclasse (Não pode ser utilizada com Relu, mas ok na output com softmax)\n",
    "    input:\n",
    "        Y: Labels\n",
    "        Y_pred: Labels Preditas\n",
    "    output:\n",
    "        custo: erro médio da predição\n",
    "'''\n",
    "def multiclass_cross_entropy_loss(Y, Y_pred):\n",
    "\n",
    "    aux = np.sum(np.multiply(Y, np.log(Y_pred)))\n",
    "    samples_number = Y.shape[1]\n",
    "    custo = -(1/samples_number) * aux\n",
    "\n",
    "    return custo\n",
    "    \n",
    "\n",
    "'''Funcao para calcular a probabilidade de ser de cada classe\n",
    "    input:\n",
    "        z: entrada da camada de saída\n",
    "    output:\n",
    "        p: probabilidade de ser de cada classe\n",
    "'''\n",
    "def softmax(z):\n",
    "    return np.exp(z) / np.sum(np.exp(z), axis=0)\n",
    "\n",
    "\n",
    "'''Funcao para calcular a matriz de confusão\n",
    "    input:\n",
    "        Y: labels corretas\n",
    "        Y_pred: labels preditas\n",
    "        classes: numéro de classes\n",
    "    output:\n",
    "        confusion_matrix: matriz de confusão\n",
    "'''\n",
    "def get_confusion_matrix(Y, Y_pred, classes):\n",
    "    \n",
    "    if(Y_pred.max() != 1):\n",
    "        Y_pred = binarize_labels(Y_pred)\n",
    "    \n",
    "    confusion_matrix = np.zeros((classes,classes)).astype(int)\n",
    "    \n",
    "    for y in range (Y.shape[1]):\n",
    "        for c_pred in range (classes):\n",
    "            if(Y_pred[c_pred, y] == 1):\n",
    "                for c in range (classes):\n",
    "                    if(Y[c, y] == 1):\n",
    "                        confusion_matrix[c, c_pred]+=1\n",
    "    \n",
    "    return confusion_matrix\n",
    "\n",
    "'''Funcao para converter um vetor de probabilidades de classes em one-hot-encoding\n",
    "    input:\n",
    "        L: vetor de probabilidade das labels\n",
    "    output:\n",
    "        Y_pred: vetor de labels (one-hot-encoding) \n",
    "    Ex: \n",
    "        Y_pred[:,1] = [0,1,0,0,0,0,0,0,0,0] quer dizer que a classe da sample 1 é 2\n",
    "'''\n",
    "def binarize_labels(L):\n",
    "    samples_number = L.shape[1]\n",
    "    classes = L.shape[0]\n",
    "    \n",
    "    Y_pred = np.argmax(L, axis=0).reshape(1, samples_number)\n",
    "\n",
    "    y_aux = np.eye(classes)[Y_pred.astype('int32')]\n",
    "    y_aux = y_aux.T.reshape(classes, samples_number)\n",
    "\n",
    "    Y_pred = y_aux\n",
    "    \n",
    "    return Y_pred\n",
    "\n",
    "'''Funcao para calcular a predição (Feed forward) de um dataframe dados parâmetros da rede neural com 1 camada escondida\n",
    "    input:\n",
    "        weights_1: vetor de pesos para calculo da entrada da camada escondida\n",
    "        weights_2: vetor de pesos para calculo da entrada da camada de saída\n",
    "        b1: vetor de pesos do bias para calculo da entrada da camada escondida\n",
    "        b2: vetor de pesos do bias para calculo da entrada da camada de saída\n",
    "        df: Dataframe a ser predito\n",
    "        activation_function: qual função de ativação será usada nos neurônios (relu, sigmoid, tanh)\n",
    "    output:\n",
    "        ol_output: vetor contendo as labels preditas\n",
    "'''\n",
    "def predict_labels_1_hidden(weights_1, weights_2, b1, b2, df, activation_function):\n",
    "    \n",
    "    hl_input = np.matmul(weights_1,df.T) + b1\n",
    "    hl_output = activation_function(hl_input)\n",
    "    ol_input = np.matmul(weights_2,hl_output) + b2\n",
    "    ol_output = softmax(ol_input)\n",
    "    \n",
    "    return ol_output\n",
    "\n",
    "'''Funcao para calcular a predição (Feed forward) de um dataframe dados parâmetros de uma rede neural com 2 camadas escondidas\n",
    "    input:\n",
    "        weights_1: vetor de pesos para calculo da entrada da primeira camada escondida\n",
    "        weights_2: vetor de pesos para calculo da entrada da segunda camada escondida\n",
    "        weights_2: vetor de pesos para calculo da entrada da camada de saída\n",
    "        b1: vetor de pesos do bias para calculo da entrada da primeira camada escondida\n",
    "        b1: vetor de pesos do bias para calculo da entrada da segunda camada escondida\n",
    "        b2: vetor de pesos do bias para calculo da entrada da camada de saída\n",
    "        df: Dataframe a ser predito\n",
    "        activation_function: qual função de ativação será usada nos neurônios (relu, sigmoid, tanh)\n",
    "    output:\n",
    "        ol_output: vetor contendo as labels preditas\n",
    "'''\n",
    "def predict_labels_2_hidden(weights_1, weights_2, weights_3, b1, b2, b3, df, activation_function):\n",
    "    \n",
    "    hl_1_input = np.matmul(weights_1,normalized_validation_df.T) + b1\n",
    "    hl_1_output = activation_function(hl_1_input) \n",
    "    hl_2_input = np.matmul(weights_2,hl_1_output) + b2\n",
    "    hl_2_output = activation_function(hl_2_input)\n",
    "    ol_input = np.matmul(weights_3,hl_2_output) + b3\n",
    "    ol_output = softmax(ol_input)\n",
    "    \n",
    "    return ol_output\n",
    "\n",
    "'''Funcao para obter as métricas de performance: precision, recall e f1_score\n",
    "    input:\n",
    "        confusion_matrix: matriz de confusão (use get_confusion_matrix() para calculá-la)\n",
    "    output:\n",
    "        precision: relação entre a quantidade de positivos preditos pela quantidade real de positivos \n",
    "        recall: relação entre a quantidade de positivos esperados pela quantidade de positivos preditos\n",
    "        f1_score: metrica para relacionar precision e recall em uma única métrica\n",
    "'''\n",
    "def get_metrics(confusion_matrix):\n",
    "    precision = get_precision(confusion_matrix)\n",
    "    recall = get_recall(confusion_matrix)\n",
    "    f1_score = get_f1_score(precision, recall)\n",
    "    \n",
    "    return precision, recall, f1_score\n",
    "\n",
    "def print_metrics(precision, recall, f1_score):\n",
    "    precision = np.around(precision, decimals=2).reshape( precision.shape[0],1)\n",
    "    recall = np.around(recall, decimals=2).reshape( recall.shape[0],1)\n",
    "    f1_score = np.around(f1_score, decimals=2).reshape( f1_score.shape[0],1)\n",
    "    \n",
    "    average_precision = np.sum(precision)/precision.shape[0]\n",
    "    average_recall = np.sum(recall)/recall.shape[0]\n",
    "    average_f1_score = np.sum(f1_score)/f1_score.shape[0]\n",
    "    \n",
    "    print(\"\\n\\nPrecision (Pr), Recall (Re) and F1_Score (F1) of each class: \")\n",
    "    print(\"Pr   Re    F1\")\n",
    "    print(re.sub(r' *\\n *', '\\n', np.array_str(np.c_[precision, recall, f1_score]).replace('[', '').replace(']', '').strip()))\n",
    "    \n",
    "    print(\"\\n\\nAverage Precision: \", round(average_precision,2) , \"\\nAverage Recall: \", round(average_recall, 2) ,\"\\nAverage F1_Score: \", round(average_f1_score,2))\n",
    "    \n",
    "    #print(precision,\" \", recall,\" \", f1_score)\n",
    "\n",
    "def get_recall(confusion_matrix):\n",
    "    precision = np.ones((confusion_matrix.shape[0]))\n",
    "    for i in range (confusion_matrix.shape[0]):\n",
    "        tp = confusion_matrix[i,i]\n",
    "        tp_fp = np.sum(confusion_matrix[:,i])\n",
    "        \n",
    "        precision[i] = tp/tp_fp\n",
    "        \n",
    "    return precision\n",
    "\n",
    "def get_precision(confusion_matrix):\n",
    "    recall = np.ones(confusion_matrix.shape[0])\n",
    "    \n",
    "    for i in range (confusion_matrix.shape[0]):\n",
    "        tp = confusion_matrix[i,i]\n",
    "        tp_fn = np.sum(confusion_matrix[i,:])\n",
    "        \n",
    "        recall[i] = tp/tp_fn\n",
    "        \n",
    "    return recall\n",
    "\n",
    "def get_f1_score(precision, recall):\n",
    "    f1 = 2*( (precision*recall) / (precision+recall) )\n",
    "    return f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparando os Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Dataframes......\n",
      "Normalizing Training(This may take a couple minutes).....\n",
      "Normalizing Test (This may take a couple minutes).....\n",
      "Preparing the labels.....\n",
      "All done!.....\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading Dataframes......\")\n",
    "train_df = pd.read_csv(\"data/fashion-mnist_train.csv\")\n",
    "validation_df = pd.read_csv(\"data/fashion-mnist_test.csv\")\n",
    "\n",
    "#train_df = df\n",
    "#validation_df = df_test\n",
    "\n",
    "Y_train = train_df[\"label\"]\n",
    "Y_validation = validation_df[\"label\"]\n",
    "\n",
    "train_df = train_df.loc[:, train_df.columns != \"label\"]\n",
    "validation_df = validation_df.loc[:, validation_df.columns != \"label\"]\n",
    "\n",
    "#test_df = pd.read_csv(\"data/fashion-mnist_test.csv\")\n",
    "\n",
    "print(\"Normalizing Training(This may take a couple minutes).....\")\n",
    "normalized_train_df = normalize_dataframe(train_df)\n",
    "print(\"Normalizing Test (This may take a couple minutes).....\")\n",
    "normalized_validation_df = normalize_dataframe(validation_df)\n",
    "\n",
    "\n",
    "print(\"Preparing the labels.....\")\n",
    "#One hot encoding labels para o softmax\n",
    "classes = 10\n",
    "\n",
    "samples_train = Y_train.shape[0]\n",
    "samples_validation = Y_validation.shape[0]\n",
    "\n",
    "Y_train = np.array(Y_train).reshape(1, samples_train)\n",
    "\n",
    "y_aux = np.eye(classes)[Y_train.astype('int32')]\n",
    "y_aux = y_aux.T.reshape(classes, samples_train)\n",
    "Y_train = y_aux\n",
    "\n",
    "Y_validation = np.array(Y_validation).reshape(1, samples_validation)\n",
    "\n",
    "y_aux = np.eye(classes)[Y_validation.astype('int32')]\n",
    "y_aux = y_aux.T.reshape(classes, samples_validation)\n",
    "\n",
    "Y_validation = y_aux\n",
    "print(\"All done!.....\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Treinando o Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 cost:  2.3026899735371384\n",
      "Epoch 100 cost:  0.7133677851712653\n",
      "Epoch 200 cost:  0.5654070175634666\n",
      "Epoch 300 cost:  0.5161634302373893\n",
      "Epoch 400 cost:  0.47887338911400285\n",
      "Epoch 500 cost:  0.44229183765901714\n",
      "Epoch 600 cost:  0.42759786852339027\n",
      "Epoch 700 cost:  0.40928161885912673\n",
      "Epoch 800 cost:  0.40355411644585715\n",
      "Epoch 900 cost:  0.40781106603377365\n",
      "Epoch 1000 cost:  0.38972537793114564\n",
      "Epoch 1100 cost:  0.38037962088158933\n",
      "Epoch 1200 cost:  0.38280835541419\n",
      "Epoch 1300 cost:  0.3783535131555865\n",
      "Epoch 1400 cost:  0.3626692708734664\n",
      "Epoch 1500 cost:  0.3599116917423071\n",
      "Epoch 1600 cost:  0.35222141795379835\n",
      "Epoch 1700 cost:  0.3465061739849266\n",
      "Epoch 1800 cost:  0.35122699220837605\n",
      "Epoch 1900 cost:  0.3506795255832515\n",
      "Epoch 2000 cost:  0.33140915801272436\n",
      "Epoch 2100 cost:  0.34128882335282934\n",
      "Epoch 2200 cost:  0.32746487580053474\n",
      "Epoch 2300 cost:  0.3296365386030514\n",
      "Epoch 2400 cost:  0.3208962021032262\n",
      "Epoch 2500 cost:  0.3286769080693203\n",
      "Epoch 2600 cost:  0.3275264655800329\n",
      "Epoch 2700 cost:  0.3168445952712301\n",
      "Epoch 2800 cost:  0.31209124319766157\n",
      "Epoch 2900 cost:  0.3243711091579663\n",
      "Final cost: 0.30435330548702455\n"
     ]
    }
   ],
   "source": [
    "#ONE HIDDEN LAYER\n",
    "\n",
    "classes = 10\n",
    "n_hl = 32\n",
    "learning_rate = 0.3\n",
    "\n",
    "# Can be relu, sigmoid or tanh\n",
    "activation_function = relu\n",
    "#This is the derivative of the activation function denoted by <name>_\n",
    "activation_function_ = relu_\n",
    "\n",
    "iterations = 3000\n",
    "\n",
    "df = train_df.T\n",
    "Y = Y_train\n",
    "\n",
    "n_x = df.shape[0]\n",
    "m = df.shape[1]\n",
    "\n",
    "weights_1 = np.random.randn(n_hl, n_x) * 0.01\n",
    "b1 = np.zeros((n_hl, 1))\n",
    "weights_2 = np.random.randn(classes, n_hl) *0.01\n",
    "b2 = np.zeros((classes, 1))\n",
    "\n",
    "start_time = time.time()\n",
    "for i in range(iterations):\n",
    "\n",
    "    #Feed forward\n",
    "    hl_input = np.matmul(weights_1,df) + b1\n",
    "    hl_output = activation_function(hl_input)\n",
    "    ol_input = np.matmul(weights_2,hl_output) + b2\n",
    "    ol_output = softmax(ol_input)\n",
    "\n",
    "    #Calculate the error\n",
    "    cost = multiclass_cross_entropy_loss(Y, ol_output)\n",
    "\n",
    "    #Backpropagation\n",
    "    d_ol_input = ol_output-Y\n",
    "    d_weights_2 = (1./m) * np.matmul(d_ol_input, hl_output.T)\n",
    "    d_b2 = (1./m) * np.sum(d_ol_input, axis=1, keepdims=True)\n",
    "\n",
    "    d_hl_output = np.matmul(weights_2.T, d_ol_input)\n",
    "    d_hl_input = d_hl_output * activation_function_(hl_input)\n",
    "    d_weights_1 = (1./m) * np.matmul(d_hl_input, df.T)\n",
    "    d_b1 = (1./m) * np.sum(d_hl_input, axis=1, keepdims=True)\n",
    "\n",
    "    #Atualização dos pesos e biases\n",
    "    weights_2 = weights_2 - learning_rate * d_weights_2\n",
    "    b2 = b2 - learning_rate * d_b2\n",
    "    weights_1 = weights_1 - learning_rate * d_weights_1\n",
    "    b1 = b1 - learning_rate * d_b1\n",
    "\n",
    "    if (i % 100 == 0):\n",
    "        print(\"Epoch\", i, \"cost: \", cost)\n",
    "\n",
    "final_time = time.time() - start_time\n",
    "print(\"Final cost:\", cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testando o Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[820   2   9  25   0   2 133   0   9   0]\n",
      " [  1 974   1  18   0   2   4   0   0   0]\n",
      " [ 13   0 807  14  68   1  95   0   2   0]\n",
      " [ 29  14   9 908  15   1  23   0   1   0]\n",
      " [  2   0  96  35 754   1 110   0   2   0]\n",
      " [  0   0   1   1   0 937   0  39   5  17]\n",
      " [131   4  70  22  37   0 726   0  10   0]\n",
      " [  0   0   0   0   0  31   0 919   0  50]\n",
      " [  3   1   6   3   3   5  13   4 961   1]\n",
      " [  0   0   0   0   0  13   0  33   2 952]]\n",
      "\n",
      "\n",
      "Precision (Pr), Recall (Re) and F1_Score (F1) of each class: \n",
      "Pr   Re    F1\n",
      "0.82 0.82 0.82\n",
      "0.97 0.98 0.98\n",
      "0.81 0.81 0.81\n",
      "0.91 0.88 0.9\n",
      "0.75 0.86 0.8\n",
      "0.94 0.94 0.94\n",
      "0.73 0.66 0.69\n",
      "0.92 0.92 0.92\n",
      "0.96 0.97 0.96\n",
      "0.95 0.93 0.94\n",
      "\n",
      "\n",
      "Average Precision:  0.88 \n",
      "Average Recall:  0.88 \n",
      "Average F1_Score:  0.88\n"
     ]
    }
   ],
   "source": [
    "prediction = predict_labels_1_hidden(weights_1, weights_2, b1, b2, normalized_validation_df, activation_function)\n",
    "\n",
    "samples_number = prediction.shape[1]\n",
    "\n",
    "predictions = np.argmax(prediction, axis=0)\n",
    "labels = np.argmax(Y_validation, axis=0)\n",
    "\n",
    "confusion_matrix = get_confusion_matrix(Y_validation, prediction, classes)\n",
    "\n",
    "print(confusion_matrix)\n",
    "\n",
    "precision, recall, f1_score = get_metrics(confusion_matrix)\n",
    "\n",
    "print_metrics(precision, recall, f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tempo gasto:  430.2525317668915\n"
     ]
    }
   ],
   "source": [
    "print(\"Tempo gasto: \", final_time)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
