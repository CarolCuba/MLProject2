{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.set_printoptions(threshold=np.nan)\n",
    "from sklearn.datasets import fetch_mldata\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import time\n",
    "import re\n",
    "\n",
    "#-------------MISC------------------\n",
    "\n",
    "def change_label(which_class,y_train,y_test):\n",
    "\n",
    "    samples_train = y_train.shape[1]\n",
    "    samples_test = y_test.shape[1]\n",
    "    \n",
    "    new_labels_train = np.zeros((1,samples_train ))\n",
    "    new_labels_test = np.zeros((1,samples_test))\n",
    "\n",
    "    ind_row_tr, ind_col_tr = np.where(y_train == which_class)\n",
    "    ind_row_tst, ind_col_tst = np.where(y_test== which_class)\n",
    "    \n",
    "    new_labels_train[ind_row_tr,ind_col_tr] = 1\n",
    "    new_labels_test[ind_row_tst,ind_col_tst] = 1\n",
    "    \n",
    "    return new_labels_train, new_labels_test\n",
    "\n",
    "\n",
    "'''Criando a funcao de normalização de um dataframe inteiro\n",
    "    input:\n",
    "        df: Dataframe\n",
    "    output:\n",
    "        df: Dataframe com valores normalizados\n",
    "'''\n",
    "def normalize_dataframe(df):\n",
    "    for column in df:\n",
    "        df[column] = df[column]/255\n",
    "    return df\n",
    "\n",
    "\n",
    "'''Funcao para converter um vetor de probabilidades de classes em one-hot-encoding\n",
    "    input:\n",
    "        L: vetor de probabilidade das labels\n",
    "    output:\n",
    "        Y_pred: vetor de labels (one-hot-encoding) \n",
    "    Ex: \n",
    "        Y_pred[:,1] = [0,1,0,0,0,0,0,0,0,0] quer dizer que a classe da sample 1 é 2\n",
    "'''\n",
    "def binarize_labels(L, classes):\n",
    "    samples_number = L.shape[0]\n",
    "    \n",
    "    Y_pred = np.array(L).reshape(1, samples_number)\n",
    "\n",
    "    y_aux = np.eye(classes)[Y_pred.astype('int32')]\n",
    "    y_aux = y_aux.T.reshape(classes, samples_number)\n",
    "\n",
    "    Y_pred = y_aux\n",
    "    \n",
    "    return Y_pred\n",
    "\n",
    "#-------------MISC------------------\n",
    "\n",
    "    \n",
    "    \n",
    "#-------------TRAINING------------------\n",
    "\n",
    "'''Criando funcao para inicializar matriz de pesos e biases nos formatos corretos\n",
    "    input:\n",
    "        df: Dataframe\n",
    "    output:\n",
    "        df: Dataframe com valores normalizados\n",
    "'''\n",
    "def initialize_parameters(dim,num_classes):\n",
    "    w = np.zeros((dim,10))\n",
    "    b = np.zeros(10)\n",
    "    return w,b\n",
    "\n",
    "\n",
    "'''Criando funcao para calcular o custo do modelo\n",
    "    input:\n",
    "        w: matriz de pesos (featuresXn_classes)\n",
    "        b: vetor de biases, um para cada classe\n",
    "        X: matriz de features de treino\n",
    "        Y: vetor de labels-Ground Truth\n",
    "        m: quantidade de amostras\n",
    "    output:\n",
    "        dw: matriz do residuo calculado para os pesos\n",
    "        db: vetor do residuo calculado para os biases\n",
    "        cost: custo gerado pelo modelo naquela iteracao\n",
    "'''\n",
    "def cost_function(w, b, X, Y,m):\n",
    "\n",
    "    z =np.matmul(X,w)+b #hipotese without sigmoid\n",
    "    A = softmax(z) # applying hipotese on sigmoid, A is the result of softmax activations\n",
    "    cost = cross_entropy(A,Y) #find cost values for all samples\n",
    "   \n",
    "    dw = (1.0/m)*(np.matmul((A-Y),X)) #find residual of weights\n",
    "    db = (1.0/m)*(np.sum(A-Y)) # find residual of bias\n",
    "   \n",
    "    return dw.T,db.T, cost\n",
    "\n",
    "\n",
    "\n",
    "'''Criando funcao para calcular o custo do modelo\n",
    "    input:\n",
    "        w: matriz de pesos (featuresXn_classes)\n",
    "        b: vetor de biases, um para cada classe\n",
    "        X: matriz de features de treino\n",
    "        Y: vetor de labels-Ground Truth\n",
    "        num_iterations: numero de iteracoes\n",
    "        alpha: learning rate\n",
    "        print_cost: flag para retornar ou nao o valor de todos os custos computados\n",
    "        samples: quantidade de amostras\n",
    "    output:\n",
    "        w: matriz dos pesos finais \n",
    "        b: vetor dos biases finais\n",
    "        cost_array: vetor de custos\n",
    "'''\n",
    "def gradient_descent(w,b,X,Y,num_iterations,alpha, print_cost,samples):\n",
    "    cost_array = []\n",
    "    mydict_costs={} \n",
    "    for i in range(num_iterations):\n",
    "        #call cost function\n",
    "        dw,db,cost = cost_function(w,b,X,Y,samples)\n",
    "        w = w - alpha*dw\n",
    "        b = b - alpha*db\n",
    "        \n",
    "        #save cost\n",
    "       \n",
    "        \n",
    "        if (print_cost == True and i%50 == 0):\n",
    "            print(\"Cost from iteration \",i,\"= \",cost,\"\\n\")\n",
    "            cost_array.append(cost)\n",
    "            mydict_costs.update({i:cost})\n",
    "            \n",
    "    return w,b,mydict_costs\n",
    "\n",
    "\n",
    "\n",
    "'''Funcao calcular os valores das precições utilizando a funcao softmax\n",
    "    input:\n",
    "        y_linear: matriz resultante da multiplicacao dos pesos pelos features, adicionando o vetore de biases (calculo da hipotese)\n",
    "    output:\n",
    "       ativacoes de classe geradas pelo softmax para cada sample\n",
    "'''\n",
    "def softmax(y_linear):\n",
    "    exp = np.exp(y_linear)\n",
    "    norms = np.sum(exp, axis=1)\n",
    "    return exp.T / norms\n",
    "\n",
    "\n",
    "\n",
    "'''Funcao obter o custo dado a funcao cross-entropy\n",
    "    input:\n",
    "       yhat: vetor de predicoes\n",
    "       y: vetor de labels ground truth\n",
    "    output:\n",
    "        erro daqueles determinados pesos calculado pela funcao cross entropy\n",
    "'''\n",
    "def cross_entropy(yhat, y):\n",
    "    return - (np.sum(y * np.log(yhat+1e-6) + (1-y)*np.log(1-yhat+1e-6)))/y.shape[0]\n",
    "\n",
    "\n",
    "\n",
    "#-------------TRAINING------------------\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#-------------METRICS------------------\n",
    "\n",
    "\n",
    "'''Funcao para obter as métricas de performance: precision, recall e f1_score\n",
    "    input:\n",
    "        confusion_matrix: matriz de confusão (use get_confusion_matrix() para calculá-la)\n",
    "    output:\n",
    "        precision: relação entre a quantidade de positivos preditos pela quantidade real de positivos \n",
    "        recall: relação entre a quantidade de positivos esperados pela quantidade de positivos preditos\n",
    "        f1_score: metrica para relacionar precision e recall em uma única métrica\n",
    "'''\n",
    "def get_metrics(confusion_matrix):\n",
    "    precision = get_precision(confusion_matrix)\n",
    "    recall = get_recall(confusion_matrix)\n",
    "    f1_score = get_f1_score(precision, recall)\n",
    "    \n",
    "    return precision, recall, f1_score\n",
    "\n",
    "\n",
    "def print_metrics(precision, recall, f1_score):\n",
    "    precision = np.around(precision, decimals=2).reshape( precision.shape[0],1)\n",
    "    recall = np.around(recall, decimals=2).reshape( recall.shape[0],1)\n",
    "    f1_score = np.around(f1_score, decimals=2).reshape( f1_score.shape[0],1)\n",
    "    \n",
    "    average_precision = np.sum(precision)/precision.shape[0]\n",
    "    average_recall = np.sum(recall)/recall.shape[0]\n",
    "    average_f1_score = np.sum(f1_score)/f1_score.shape[0]\n",
    "    \n",
    "    print(\"\\n\\nPrecision (Pr), Recall (Re) and F1_Score (F1) of each class: \")\n",
    "    print(\"Pr   Re    F1\")\n",
    "    print(re.sub(r' *\\n *', '\\n', np.array_str(np.c_[precision, recall, f1_score]).replace('[', '').replace(']', '').strip()))\n",
    "    \n",
    "    print(\"\\n\\nAverage Precision: \", round(average_precision,2) , \"\\nAverage Recall: \", round(average_recall, 2) ,\"\\nAverage F1_Score: \", round(average_f1_score,2))\n",
    "    \n",
    "\n",
    "def get_recall(confusion_matrix):\n",
    "    precision = np.ones((confusion_matrix.shape[0]))\n",
    "    for i in range (confusion_matrix.shape[0]):\n",
    "        tp = confusion_matrix[i,i]\n",
    "        tp_fp = np.sum(confusion_matrix[:,i])\n",
    "        \n",
    "        precision[i] = tp/tp_fp\n",
    "        \n",
    "    return precision\n",
    "\n",
    "\n",
    "def get_precision(confusion_matrix):\n",
    "    recall = np.ones(confusion_matrix.shape[0])\n",
    "    \n",
    "    for i in range (confusion_matrix.shape[0]):\n",
    "        tp = confusion_matrix[i,i]\n",
    "        tp_fn = np.sum(confusion_matrix[i,:])\n",
    "        \n",
    "        recall[i] = tp/tp_fn\n",
    "        \n",
    "    return recall\n",
    "\n",
    "\n",
    "def get_f1_score(precision, recall):\n",
    "    f1 = 2*( (precision*recall) / (precision+recall) )\n",
    "    return f1\n",
    "\n",
    "\n",
    "#-------------METRICS------------------\n",
    "\n",
    "\n",
    "\n",
    "#-------------Ploting------------------\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "'''Funcao para calcular a matriz de confusão\n",
    "    input:\n",
    "        Y: labels corretas\n",
    "        Y_pred: labels preditas\n",
    "        classes: numéro de classes\n",
    "    output:\n",
    "        confusion_matrix: matriz de confusão\n",
    "'''\n",
    "def get_confusion_matrix(Y, Y_pred, classes):\n",
    "    \n",
    "    if(Y_pred.max() != 1):\n",
    "        Y_pred = binarize_labels(Y_pred,classes)\n",
    "        \n",
    "    if(Y.max() != 1 ):\n",
    "        Y = binarize_labels(Y,classes)\n",
    "    \n",
    "    confusion_matrix = np.zeros((classes,classes)).astype(int)\n",
    "    \n",
    "    for y in range (Y.shape[1]):\n",
    "        for c_pred in range (classes):\n",
    "            if(Y_pred[c_pred, y] == 1):\n",
    "                for c in range (classes):\n",
    "                    if(Y[c, y] == 1):\n",
    "                        confusion_matrix[c, c_pred]+=1\n",
    "    \n",
    "    return confusion_matrix\n",
    "\n",
    "#-------------Ploting------------------\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Dataframes......\n",
      "Separating into training and validation.....\n",
      "Normalizing (This may take a couple minutes).....\n",
      "Preparing the labels.....\n",
      "784\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading Dataframes......\")\n",
    "df = pd.read_csv(\"data/fashion-mnist_train.csv\")\n",
    "\n",
    "print(\"Separating into training and validation.....\")\n",
    "#Separate the Training DF into Train and Validation\n",
    "msk = np.random.rand(len(df)) < 0.7 \n",
    "\n",
    "train_df = df[msk]\n",
    "validation_df = df[~msk]\n",
    "\n",
    "Y_train = train_df[\"label\"]\n",
    "Y_validation = validation_df[\"label\"]\n",
    "\n",
    "train_df = train_df.loc[:, train_df.columns != \"label\"]\n",
    "validation_df = validation_df.loc[:, validation_df.columns != \"label\"]\n",
    "\n",
    "#test_df = pd.read_csv(\"data/fashion-mnist_test.csv\")\n",
    "\n",
    "print(\"Normalizing (This may take a couple minutes).....\")\n",
    "normalized_train_df = normalize_dataframe(train_df)\n",
    "normalized_validation_df = normalize_dataframe(validation_df)\n",
    "\n",
    "\n",
    "print(\"Preparing the labels.....\")\n",
    "#One hot encoding labels para o softmax\n",
    "classes = 10\n",
    "\n",
    "samples_train = Y_train.shape[0]\n",
    "samples_validation = Y_validation.shape[0]\n",
    "\n",
    "Y_train = np.array(Y_train).reshape(1, samples_train)\n",
    "\n",
    "y_aux = np.eye(classes)[Y_train.astype('int32')]\n",
    "y_aux = y_aux.T.reshape(classes, samples_train)\n",
    "Y_train = y_aux\n",
    "\n",
    "Y_validation = np.array(Y_validation).reshape(1, samples_validation)\n",
    "\n",
    "y_aux = np.eye(classes)[Y_validation.astype('int32')]\n",
    "y_aux = y_aux.T.reshape(classes, samples_validation)\n",
    "\n",
    "Y_validation = y_aux\n",
    "Y_validation =Y_validation.T\n",
    "\n",
    "print(normalized_train_df.shape[1])\n",
    "\n",
    "print(\"Done!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create Testing values, this may take some minutes\n",
      "Cost from iteration  0 =  13629.995052589573 \n",
      "\n",
      "Cost from iteration  50 =  5827.8150215554 \n",
      "\n",
      "Cost from iteration  100 =  5086.096193721556 \n",
      "\n",
      "Cost from iteration  150 =  4729.81660347897 \n",
      "\n",
      "Cost from iteration  200 =  4506.5209558308 \n",
      "\n",
      "Cost from iteration  250 =  4348.85606714359 \n",
      "\n",
      "Cost from iteration  300 =  4229.458915595504 \n",
      "\n",
      "Cost from iteration  350 =  4134.735586872308 \n",
      "\n",
      "Cost from iteration  400 =  4057.044137878621 \n",
      "\n",
      "Cost from iteration  450 =  3991.706979820591 \n",
      "\n",
      "Cost from iteration  500 =  3935.676367802348 \n",
      "\n",
      "Cost from iteration  550 =  3886.8689706151827 \n",
      "\n",
      "Cost from iteration  600 =  3843.8054674888103 \n",
      "\n",
      "Cost from iteration  650 =  3805.4023344958287 \n",
      "\n",
      "Cost from iteration  700 =  3770.845162854672 \n",
      "\n",
      "Cost from iteration  750 =  3739.508242974407 \n",
      "\n",
      "Cost from iteration  800 =  3710.901677861131 \n",
      "\n",
      "Cost from iteration  850 =  3684.6355415504977 \n",
      "\n",
      "Cost from iteration  900 =  3660.3949537415638 \n",
      "\n",
      "Cost from iteration  950 =  3637.922351766237 \n",
      "\n",
      "Cost from iteration  1000 =  3617.0046295437 \n",
      "\n",
      "Cost from iteration  1050 =  3597.4636417082156 \n",
      "\n",
      "Cost from iteration  1100 =  3579.149080881317 \n",
      "\n",
      "Cost from iteration  1150 =  3561.933058306331 \n",
      "\n",
      "Cost from iteration  1200 =  3545.705926709049 \n",
      "\n",
      "Cost from iteration  1250 =  3530.373022267538 \n",
      "\n",
      "Cost from iteration  1300 =  3515.852095650492 \n",
      "\n",
      "Cost from iteration  1350 =  3502.0712659554433 \n",
      "\n",
      "Cost from iteration  1400 =  3488.967375909319 \n",
      "\n",
      "Cost from iteration  1450 =  3476.4846581943807 \n",
      "\n",
      "Cost from iteration  1500 =  3464.5736453440463 \n",
      "\n",
      "Cost from iteration  1550 =  3453.1902720416424 \n",
      "\n",
      "Cost from iteration  1600 =  3442.2951306855225 \n",
      "\n",
      "Cost from iteration  1650 =  3431.8528500096522 \n",
      "\n",
      "Cost from iteration  1700 =  3421.8315732369038 \n",
      "\n",
      "Cost from iteration  1750 =  3412.2025173009883 \n",
      "\n",
      "Cost from iteration  1800 =  3402.9395985323986 \n",
      "\n",
      "Cost from iteration  1850 =  3394.019113173045 \n",
      "\n",
      "Cost from iteration  1900 =  3385.4194633861357 \n",
      "\n",
      "Cost from iteration  1950 =  3377.12092122593 \n",
      "\n",
      "--- 349.34502482414246 seconds ---\n"
     ]
    }
   ],
   "source": [
    "#Training/testing\n",
    "\n",
    "ws, bs = initialize_parameters(normalized_train_df.shape[1],num_classes=10) #CREATING NEW WEIGHTS AND BIAS\n",
    "\n",
    "print(\"Create Testing values, this may take some minutes\")\n",
    "start_time1 = time.time()\n",
    "w,b,costs1 = gradient_descent(ws,bs,train_df,Y_train,500,0.3,True,samples_train) # pesos e bias treinados\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time1))\n",
    "\n",
    "# start_time2 = time.time()\n",
    "# w,b,costs2 = gradient_descent(ws,bs,train_df,Y_train,2000,0.1,True,samples_train) # pesos e bias treinados\n",
    "# print(\"--- %s seconds ---\" % (time.time() - start_time2))\n",
    "\n",
    "# start_time3 = time.time()\n",
    "# w,b,costs3 = gradient_descent(ws,bs,train_df,Y_train,2000,0.05,True,samples_train) # pesos e bias treinados\n",
    "# print(\"--- %s seconds ---\" % (time.time() - start_time3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 18072)\n",
      "[[1448   15   33  102    8    0  141    0   13    0]\n",
      " [   6 1731   20   54    7    0   11    0    1    0]\n",
      " [  27    6 1326   12  276    3  141    0   16    0]\n",
      " [  82   27   15 1568   53    0   48    0   10    0]\n",
      " [   7    3  159   72 1401    0  166    0   11    0]\n",
      " [   1    1    0    2    0 1610    0  113   17   99]\n",
      " [ 293    4  218   51  198    0 1008    0   30    0]\n",
      " [   0    0    0    0    0   45    0 1667    3   94]\n",
      " [   7    1   12   25   10   10   47   10 1711    3]\n",
      " [   1    0    0    0    0   27    0   67    2 1666]]\n",
      "\n",
      "\n",
      "Precision (Pr), Recall (Re) and F1_Score (F1) of each class: \n",
      "Pr   Re    F1\n",
      "0.82 0.77 0.8\n",
      "0.95 0.97 0.96\n",
      "0.73 0.74 0.74\n",
      "0.87 0.83 0.85\n",
      "0.77 0.72 0.74\n",
      "0.87 0.95 0.91\n",
      "0.56 0.65 0.6\n",
      "0.92 0.9  0.91\n",
      "0.93 0.94 0.94\n",
      "0.94 0.89 0.92\n",
      "\n",
      "\n",
      "Average Precision:  0.84 \n",
      "Average Recall:  0.84 \n",
      "Average F1_Score:  0.84\n"
     ]
    }
   ],
   "source": [
    "#predicting/tests\n",
    "z =np.matmul(validation_df,w)+b #hipotese without sigmoid\n",
    "A = softmax(z)\n",
    "sda = np.argmax(A,axis=0)\n",
    "\n",
    "y_aux = np.eye(classes)[sda.astype('int32')]\n",
    "y_aux = y_aux.T.reshape(classes, samples_validation)\n",
    "final = y_aux\n",
    "\n",
    "y_test_new = Y_validation.T\n",
    "print(y_test_new.shape)\n",
    "\n",
    "\n",
    "\n",
    "confusion_matrix = get_confusion_matrix(y_test_new,final, 10)\n",
    "\n",
    "print(confusion_matrix)\n",
    "\n",
    "precision, recall, f1_score = get_metrics(confusion_matrix)\n",
    "\n",
    "print_metrics(precision, recall, f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
