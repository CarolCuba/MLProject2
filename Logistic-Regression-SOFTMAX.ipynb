{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.set_printoptions(threshold=np.nan)\n",
    "from sklearn.datasets import fetch_mldata\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import time\n",
    "import re\n",
    "\n",
    "# changing labels to make one vs all\n",
    "\n",
    "def change_label(which_class,y_train,y_test):\n",
    "\n",
    "    samples_train = y_train.shape[1]\n",
    "    samples_test = y_test.shape[1]\n",
    "    \n",
    "    new_labels_train = np.zeros((1,samples_train ))\n",
    "    new_labels_test = np.zeros((1,samples_test))\n",
    "\n",
    "    ind_row_tr, ind_col_tr = np.where(y_train == which_class)\n",
    "    ind_row_tst, ind_col_tst = np.where(y_test== which_class)\n",
    "    \n",
    "    new_labels_train[ind_row_tr,ind_col_tr] = 1\n",
    "    new_labels_test[ind_row_tst,ind_col_tst] = 1\n",
    "    \n",
    "    return new_labels_train, new_labels_test\n",
    "\n",
    "\n",
    "'''Criando a funcao de normalização de um dataframe inteiro\n",
    "    input:\n",
    "        df: Dataframe\n",
    "    output:\n",
    "        df: Dataframe com valores normalizados\n",
    "'''\n",
    "def normalize_dataframe(df):\n",
    "    for column in df:\n",
    "        df[column] = df[column]/255\n",
    "    return df\n",
    "    \n",
    "# plot image\n",
    "def plot_image(this_digit):\n",
    "    some_digit_image = this_digit.reshape(28,28)\n",
    "    plt.imshow(some_digit_image, cmap = matplotlib.cm.binary, interpolation = \"nearest\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.show() \n",
    "\n",
    "#create sigmoid function\n",
    "def sigmoid(z):\n",
    "    s = 1.0 / (1.0 + np.exp(-z))\n",
    "    return s\n",
    "\n",
    "#initialize values of thetas and bias\n",
    "def initialize_parameters(dim):\n",
    "    w = np.zeros((784,10))\n",
    "    b = np.zeros(10)\n",
    "    return w,b\n",
    "\n",
    "def to_classlabel(z):\n",
    "    return z.argmax(axis=1)\n",
    "\n",
    "#implement cost function and its gradient propagation\n",
    "def cost_function(w, b, X, Y,m):\n",
    "\n",
    "    \n",
    "    z =np.matmul(X,w)+b #hipotese without sigmoid\n",
    "    A = softmax(z) # applying hipotese on sigmoid, A is the result of sigmoid for each sample\n",
    "    cost = cross_entropy(A,Y) #find cost values for all samples\n",
    "   \n",
    "    dw = (1.0/m)*(np.matmul((A-Y),X)) #find residual of weights\n",
    "    db = (1.0/m)*(np.sum(A-Y)) # find residual of bias\n",
    "   \n",
    "    return dw.T,db.T, cost\n",
    "\n",
    "#apply gradient descent\n",
    "def gradient_descent(w,b,X,Y,num_iterations,alpha, print_cost,samples):\n",
    "    cost_array = []\n",
    "    for i in range(num_iterations):\n",
    "        #call cost function\n",
    "        dw,db,cost = cost_function(w,b,X,Y,samples)\n",
    "        w = w - alpha*dw\n",
    "        b = b - alpha*db\n",
    "        \n",
    "        #save cost\n",
    "        cost_array.append(cost)\n",
    "        \n",
    "        if (print_cost == True and i%50 == 0):\n",
    "            print(\"Cost from iteration \",i,\"= \",cost,\"\\n\")\n",
    "            \n",
    "    return w,b,cost_array\n",
    "\n",
    "#prediction\n",
    "def prediction (w,b,X):\n",
    "    #w = w.reshape(X.shape[0],1)\n",
    "    A = np.dot(w.T, X)+b\n",
    "    return A\n",
    "\n",
    "# one vs all prediction\n",
    "def one_vs_all(w,X):\n",
    "    samples = X.shape[1]\n",
    "    weighs_qut = w.shape[0]\n",
    "    bias = w[-1]\n",
    "    weights = w[:(len(w))-1]\n",
    "    A = sigmoid (np.dot(weights.T, X)+bias)\n",
    "    \n",
    "    return A;\n",
    "\n",
    "def softmax(y_linear):\n",
    "    exp = np.exp(y_linear)\n",
    "    norms = np.sum(exp, axis=1)\n",
    "    return exp.T / norms\n",
    "\n",
    "def cross_entropy(yhat, y):\n",
    "    return - (np.sum(y * np.log(yhat+1e-6) + (1-y)*np.log(1-yhat+1e-6)))/y.shape[0]\n",
    "\n",
    "'''Funcao para obter as métricas de performance: precision, recall e f1_score\n",
    "    input:\n",
    "        confusion_matrix: matriz de confusão (use get_confusion_matrix() para calculá-la)\n",
    "    output:\n",
    "        precision: relação entre a quantidade de positivos preditos pela quantidade real de positivos \n",
    "        recall: relação entre a quantidade de positivos esperados pela quantidade de positivos preditos\n",
    "        f1_score: metrica para relacionar precision e recall em uma única métrica\n",
    "'''\n",
    "def get_metrics(confusion_matrix):\n",
    "    precision = get_precision(confusion_matrix)\n",
    "    recall = get_recall(confusion_matrix)\n",
    "    f1_score = get_f1_score(precision, recall)\n",
    "    \n",
    "    return precision, recall, f1_score\n",
    "\n",
    "def print_metrics(precision, recall, f1_score):\n",
    "    precision = np.around(precision, decimals=2).reshape( precision.shape[0],1)\n",
    "    recall = np.around(recall, decimals=2).reshape( recall.shape[0],1)\n",
    "    f1_score = np.around(f1_score, decimals=2).reshape( f1_score.shape[0],1)\n",
    "    \n",
    "    average_precision = np.sum(precision)/precision.shape[0]\n",
    "    average_recall = np.sum(recall)/recall.shape[0]\n",
    "    average_f1_score = np.sum(f1_score)/f1_score.shape[0]\n",
    "    \n",
    "    print(\"\\n\\nPrecision (Pr), Recall (Re) and F1_Score (F1) of each class: \")\n",
    "    print(\"Pr   Re    F1\")\n",
    "    print(re.sub(r' *\\n *', '\\n', np.array_str(np.c_[precision, recall, f1_score]).replace('[', '').replace(']', '').strip()))\n",
    "    \n",
    "    print(\"\\n\\nAverage Precision: \", round(average_precision,2) , \"\\nAverage Recall: \", round(average_recall, 2) ,\"\\nAverage F1_Score: \", round(average_f1_score,2))\n",
    "    \n",
    "\n",
    "def get_recall(confusion_matrix):\n",
    "    precision = np.ones((confusion_matrix.shape[0]))\n",
    "    for i in range (confusion_matrix.shape[0]):\n",
    "        tp = confusion_matrix[i,i]\n",
    "        tp_fp = np.sum(confusion_matrix[:,i])\n",
    "        \n",
    "        precision[i] = tp/tp_fp\n",
    "        \n",
    "    return precision\n",
    "\n",
    "def get_precision(confusion_matrix):\n",
    "    recall = np.ones(confusion_matrix.shape[0])\n",
    "    \n",
    "    for i in range (confusion_matrix.shape[0]):\n",
    "        tp = confusion_matrix[i,i]\n",
    "        tp_fn = np.sum(confusion_matrix[i,:])\n",
    "        \n",
    "        recall[i] = tp/tp_fn\n",
    "        \n",
    "    return recall\n",
    "\n",
    "def get_f1_score(precision, recall):\n",
    "    f1 = 2*( (precision*recall) / (precision+recall) )\n",
    "    return f1\n",
    "\n",
    "'''Funcao para calcular a matriz de confusão\n",
    "    input:\n",
    "        Y: labels corretas\n",
    "        Y_pred: labels preditas\n",
    "        classes: numéro de classes\n",
    "    output:\n",
    "        confusion_matrix: matriz de confusão\n",
    "'''\n",
    "def get_confusion_matrix(Y, Y_pred, classes):\n",
    "    \n",
    "    if(Y_pred.max() != 1):\n",
    "        Y_pred = binarize_labels(Y_pred,classes)\n",
    "        \n",
    "    if(Y.max() != 1 ):\n",
    "        Y = binarize_labels(Y,classes)\n",
    "    \n",
    "    confusion_matrix = np.zeros((classes,classes)).astype(int)\n",
    "    \n",
    "    for y in range (Y.shape[1]):\n",
    "        for c_pred in range (classes):\n",
    "            if(Y_pred[c_pred, y] == 1):\n",
    "                for c in range (classes):\n",
    "                    if(Y[c, y] == 1):\n",
    "                        confusion_matrix[c, c_pred]+=1\n",
    "    \n",
    "    return confusion_matrix\n",
    "\n",
    "\n",
    "'''Funcao para converter um vetor de probabilidades de classes em one-hot-encoding\n",
    "    input:\n",
    "        L: vetor de probabilidade das labels\n",
    "    output:\n",
    "        Y_pred: vetor de labels (one-hot-encoding) \n",
    "    Ex: \n",
    "        Y_pred[:,1] = [0,1,0,0,0,0,0,0,0,0] quer dizer que a classe da sample 1 é 2\n",
    "'''\n",
    "def binarize_labels(L, classes):\n",
    "    samples_number = L.shape[0]\n",
    "    \n",
    "    Y_pred = np.array(L).reshape(1, samples_number)\n",
    "\n",
    "    y_aux = np.eye(classes)[Y_pred.astype('int32')]\n",
    "    y_aux = y_aux.T.reshape(classes, samples_number)\n",
    "\n",
    "    Y_pred = y_aux\n",
    "    \n",
    "    return Y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Dataframes......\n",
      "Separating into training and validation.....\n",
      "Normalizing (This may take a couple minutes).....\n",
      "Preparing the labels.....\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading Dataframes......\")\n",
    "df = pd.read_csv(\"data/fashion-mnist_train.csv\")\n",
    "\n",
    "print(\"Separating into training and validation.....\")\n",
    "#Separate the Training DF into Train and Validation\n",
    "msk = np.random.rand(len(df)) < 0.7 \n",
    "\n",
    "train_df = df[msk]\n",
    "validation_df = df[~msk]\n",
    "\n",
    "Y_train = train_df[\"label\"]\n",
    "Y_validation = validation_df[\"label\"]\n",
    "\n",
    "train_df = train_df.loc[:, train_df.columns != \"label\"]\n",
    "validation_df = validation_df.loc[:, validation_df.columns != \"label\"]\n",
    "\n",
    "#test_df = pd.read_csv(\"data/fashion-mnist_test.csv\")\n",
    "\n",
    "print(\"Normalizing (This may take a couple minutes).....\")\n",
    "normalized_train_df = normalize_dataframe(train_df)\n",
    "normalized_validation_df = normalize_dataframe(validation_df)\n",
    "\n",
    "\n",
    "print(\"Preparing the labels.....\")\n",
    "#One hot encoding labels para o softmax\n",
    "classes = 10\n",
    "\n",
    "samples_train = Y_train.shape[0]\n",
    "samples_validation = Y_validation.shape[0]\n",
    "\n",
    "Y_train = np.array(Y_train).reshape(1, samples_train)\n",
    "\n",
    "y_aux = np.eye(classes)[Y_train.astype('int32')]\n",
    "y_aux = y_aux.T.reshape(classes, samples_train)\n",
    "Y_train = y_aux\n",
    "\n",
    "Y_validation = np.array(Y_validation).reshape(1, samples_validation)\n",
    "\n",
    "y_aux = np.eye(classes)[Y_validation.astype('int32')]\n",
    "y_aux = y_aux.T.reshape(classes, samples_validation)\n",
    "\n",
    "Y_validation = y_aux\n",
    "Y_validation =Y_validation.T\n",
    "\n",
    "print(\"Done!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost from iteration  0 =  13670.955255237594 \n",
      "\n",
      "Cost from iteration  50 =  6054.856657995749 \n",
      "\n",
      "Cost from iteration  100 =  5834.210095215725 \n",
      "\n",
      "Cost from iteration  150 =  5835.382871331535 \n",
      "\n",
      "Cost from iteration  200 =  4976.132575654226 \n",
      "\n",
      "Cost from iteration  250 =  5342.470231403505 \n",
      "\n",
      "Cost from iteration  300 =  4702.789119779424 \n",
      "\n",
      "Cost from iteration  350 =  4833.654889055105 \n",
      "\n",
      "Cost from iteration  400 =  4566.992620632636 \n",
      "\n",
      "Cost from iteration  450 =  4413.078712128228 \n",
      "\n",
      "Cost from iteration  500 =  4309.677843082872 \n",
      "\n",
      "Cost from iteration  550 =  4700.549725478374 \n",
      "\n",
      "Cost from iteration  600 =  4456.175988280427 \n",
      "\n",
      "Cost from iteration  650 =  4349.5520356359075 \n",
      "\n",
      "Cost from iteration  700 =  4285.035467299555 \n",
      "\n",
      "Cost from iteration  750 =  4239.971566115341 \n",
      "\n",
      "Cost from iteration  800 =  4131.8749923447 \n",
      "\n",
      "Cost from iteration  850 =  4035.168174878393 \n",
      "\n",
      "Cost from iteration  900 =  3956.9792049300477 \n",
      "\n",
      "Cost from iteration  950 =  4189.903814219751 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Training\n",
    "ws, bs = initialize_parameters(samples_train) #CREATING NEW WEIGHTS AND BIAS\n",
    "\n",
    "w,b,costs = gradient_descent(ws,bs,train_df,Y_train,1000,0.3,True,samples_train) # pesos e bias treinados\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 17946)\n",
      "[[1651   13   24   67   29    0   26    0   26    0]\n",
      " [   9 1676   10   41   15    0    1    0    2    0]\n",
      " [  41    4 1119   17  581    1   31    0   13    0]\n",
      " [ 126   21   14 1489  161    0   13    0    7    0]\n",
      " [   9    0   51   32 1737    0   20    0   10    0]\n",
      " [   4    4    1    3    0 1488    0  105   23   86]\n",
      " [ 450    4  202   57  639    0  388    1   41    0]\n",
      " [   0    0    0    0    0   45    0 1638    5   93]\n",
      " [  11    2    8   20   25   17    7   14 1736    2]\n",
      " [   1    0    0    1    0   19    0   64    3 1652]]\n",
      "\n",
      "\n",
      "Precision (Pr), Recall (Re) and F1_Score (F1) of each class: \n",
      "Pr   Re    F1\n",
      "0.9  0.72 0.8\n",
      "0.96 0.97 0.96\n",
      "0.62 0.78 0.69\n",
      "0.81 0.86 0.84\n",
      "0.93 0.55 0.69\n",
      "0.87 0.95 0.91\n",
      "0.22 0.8  0.34\n",
      "0.92 0.9  0.91\n",
      "0.94 0.93 0.94\n",
      "0.95 0.9  0.92\n",
      "\n",
      "\n",
      "Average Precision:  0.81 \n",
      "Average Recall:  0.84 \n",
      "Average F1_Score:  0.8\n"
     ]
    }
   ],
   "source": [
    "#predicting\n",
    "z =np.matmul(validation_df,w)+b #hipotese without sigmoid\n",
    "A = softmax(z)\n",
    "sda = np.argmax(A,axis=0)\n",
    "\n",
    "y_aux = np.eye(classes)[sda.astype('int32')]\n",
    "y_aux = y_aux.T.reshape(classes, samples_validation)\n",
    "final = y_aux\n",
    "\n",
    "y_test_new = Y_validation.T\n",
    "print(y_test_new.shape)\n",
    "\n",
    "confusion_matrix = get_confusion_matrix(y_test_new,final, 10)\n",
    "\n",
    "print(confusion_matrix)\n",
    "\n",
    "precision, recall, f1_score = get_metrics(confusion_matrix)\n",
    "\n",
    "print_metrics(precision, recall, f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
