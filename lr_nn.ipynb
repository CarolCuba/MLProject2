{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classificação do Fashion MNIST com NN simples e Regressão Logística"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Funções das Redes Neurais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Criando a funcao de normalização de um dataframe inteiro\n",
    "    input:\n",
    "        df: Dataframe\n",
    "    output:\n",
    "        df: Dataframe com valores normalizados\n",
    "'''\n",
    "def normalize_dataframe(df):\n",
    "    for column in df:\n",
    "        df[column] = df[column]/255\n",
    "    return df\n",
    "\n",
    "\n",
    "'''Funcao para aplicar relu em um vetor\n",
    "    input:\n",
    "        z: weights * params + bias\n",
    "    output:\n",
    "        s: 0 para valores negativos z para valores positivos aplicado em cada amostra\n",
    "'''    \n",
    "def relu(z):\n",
    "    return np.maximum(z, 0)\n",
    "\n",
    "\n",
    "'''Funcao para aplicar a derivada da relu em um vetor\n",
    "    input:\n",
    "        z: weights * params + bias\n",
    "    output:\n",
    "        s: 0 para valores negativos 1 para valores positivos aplicado em cada amostra\n",
    "'''    \n",
    "def relu_(z):\n",
    "    return np.array(z>0).astype(int)\n",
    "\n",
    "\n",
    "'''Funcao para aplicar sigmoid em um vetor\n",
    "    input:\n",
    "        z: weights * params + bias\n",
    "    output:\n",
    "        s: valor entre 0-1 da aplicação da função para cada amostra\n",
    "'''\n",
    "def sigmoid(z):\n",
    "    #print(z.shape)\n",
    "    s = 1 / (1 + np.exp(-z))\n",
    "    return s\n",
    "\n",
    "\n",
    "'''Funcao para aplicar a derivada da sigmoid em um vetor\n",
    "    input:\n",
    "        z: weights * params + bias\n",
    "    output:\n",
    "        p: taxa de variação da sigmoid para cada amostra\n",
    "'''\n",
    "def sigmoid_(z):\n",
    "    #print(sigmoid(Z1).shape)\n",
    "    s = sigmoid(z) * (1 - sigmoid(z))\n",
    "    return s\n",
    "\n",
    "\n",
    "'''Funcao para calcular a tangente hiporbólica em um vetor\n",
    "    input:\n",
    "        x: weights * params + bias\n",
    "    output:\n",
    "        s: valor entre 0-1 da aplicação da função para cada amostra\n",
    "'''\n",
    "def tanh(x):\n",
    "    s = (2 / (1 + np.exp(-2*x))) -1\n",
    "    return s\n",
    "\n",
    "\n",
    "'''Funcao para calcular a derivada da tangente hiporbólica em um vetor\n",
    "    input:\n",
    "        x: weights * params + bias\n",
    "    output:\n",
    "        s: porcentagem de chance do valor ser da classe predita para cada amostra\n",
    "'''\n",
    "def tanh_(x):\n",
    "    s = 1 - tanh(x)**2\n",
    "    return s\n",
    "\n",
    "\n",
    "'''Funcao para calcular o erro médio da predição utilizando cross-entropy (Não pode ser utilizada com Relu, mas ok na output com softmax)\n",
    "    input:\n",
    "        Y: Labels\n",
    "        Y_pred: Labels Preditas\n",
    "    output:\n",
    "        custo: erro médio da predição\n",
    "'''\n",
    "def cross_entropy_loss(Y, Y_pred):\n",
    "    samples_number = Y.shape[1]\n",
    "    custo = -(1./samples_number) * ( np.sum( np.multiply(np.log(Y_pred),Y) ) + np.sum( np.multiply(np.log(1-Y_pred),(1-Y)) ) )\n",
    "    return custo\n",
    "\n",
    "\n",
    "'''Funcao para calcular o erro médio da predição utilizando cross-entropy multiclasse (Não pode ser utilizada com Relu, mas ok na output com softmax)\n",
    "    input:\n",
    "        Y: Labels\n",
    "        Y_pred: Labels Preditas\n",
    "    output:\n",
    "        custo: erro médio da predição\n",
    "'''\n",
    "def multiclass_cross_entropy_loss(Y, Y_pred):\n",
    "\n",
    "    aux = np.sum(np.multiply(Y, np.log(Y_pred)))\n",
    "    samples_number = Y.shape[1]\n",
    "    custo = -(1/samples_number) * aux\n",
    "\n",
    "    return custo\n",
    "    \n",
    "\n",
    "'''Funcao para calcular a probabilidade de ser de cada classe\n",
    "    input:\n",
    "        z: entrada da camada de saída\n",
    "    output:\n",
    "        p: probabilidade de ser de cada classe\n",
    "'''\n",
    "def softmax(z):\n",
    "    return np.exp(z) / np.sum(np.exp(z), axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparando os Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import time\n",
    "\n",
    "df = pd.read_csv(\"data/fashion-mnist_train.csv\")\n",
    "\n",
    "#Separate the Training DF into Train and Validation\n",
    "msk = np.random.rand(len(df)) < 0.6 \n",
    "\n",
    "train_df = df[msk]\n",
    "validation_df = df[~msk]\n",
    "\n",
    "Y_train = train_df[\"label\"]\n",
    "Y_validation = validation_df[\"label\"]\n",
    "\n",
    "train_df = train_df.loc[:, train_df.columns != \"label\"]\n",
    "validation_df = validation_df.loc[:, validation_df.columns != \"label\"]\n",
    "\n",
    "#test_df = pd.read_csv(\"data/fashion-mnist_test.csv\")\n",
    "\n",
    "normalized_train_df = normalize_dataframe(train_df)\n",
    "normalized_validation_df = normalize_dataframe(validation_df)\n",
    "\n",
    "\n",
    "#One hot encoding labels para o softmax\n",
    "classes = 10\n",
    "\n",
    "samples_train = Y_train.shape[0]\n",
    "samples_validation = Y_validation.shape[0]\n",
    "\n",
    "Y_train = np.array(Y_train).reshape(1, samples_train)\n",
    "\n",
    "y_aux = np.eye(classes)[Y_train.astype('int32')]\n",
    "y_aux = y_aux.T.reshape(classes, samples_train)\n",
    "Y_train = y_aux\n",
    "\n",
    "Y_validation = np.array(Y_validation).reshape(1, samples_validation)\n",
    "\n",
    "y_aux = np.eye(classes)[Y_validation.astype('int32')]\n",
    "y_aux = y_aux.T.reshape(classes, samples_validation)\n",
    "\n",
    "Y_validation = y_aux"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualização dos Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n"
     ]
    }
   ],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "i = 3\n",
    "#print(np.array(train_df.iloc[1]).reshape(28,28))\n",
    "plt.imshow(np.array(train_df.iloc[5]).reshape(28,28), cmap = matplotlib.cm.binary)\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "print(Y_train[:,5])\n",
    "\n",
    "# 0 \tT-shirt/top\n",
    "# 1 \tTrouser\n",
    "# 2 \tPullover\n",
    "# 3 \tDress\n",
    "# 4 \tCoat\n",
    "# 5 \tSandal\n",
    "# 6 \tShirt\n",
    "# 7 \tSneaker\n",
    "# 8 \tBag\n",
    "# 9 \tAnkle boot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rede Neural com 1 Camada Escondida (Multi Class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 cost:  2.3036250511267475\n",
      "Epoch 100 cost:  1.245583603756905\n",
      "Epoch 200 cost:  0.8657819158539172\n",
      "Epoch 300 cost:  0.71017659759372\n",
      "Epoch 400 cost:  0.6363448574154088\n",
      "Epoch 500 cost:  0.5895219238766266\n",
      "Epoch 600 cost:  0.5552352213388783\n",
      "Epoch 700 cost:  0.5286422190701621\n",
      "Epoch 800 cost:  0.5075402877929316\n",
      "Epoch 900 cost:  0.4905077984538176\n",
      "Epoch 1000 cost:  0.47644502093151386\n",
      "Epoch 1100 cost:  0.4645457263638161\n",
      "Epoch 1200 cost:  0.45426011522976495\n",
      "Epoch 1300 cost:  0.4452232628286702\n",
      "Epoch 1400 cost:  0.4371864274035958\n",
      "Epoch 1500 cost:  0.4299680362699815\n",
      "Epoch 1600 cost:  0.42342663202140557\n",
      "Epoch 1700 cost:  0.41744833220934036\n",
      "Epoch 1800 cost:  0.4119408694793463\n",
      "Epoch 1900 cost:  0.4068299044569764\n",
      "Final cost: 0.4021022262800445\n"
     ]
    }
   ],
   "source": [
    "#ONE HIDDEN LAYER\n",
    "\n",
    "classes = 10\n",
    "n_hl = 64\n",
    "learning_rate = 0.3\n",
    "\n",
    "X = train_df.T\n",
    "Y = Y_train\n",
    "\n",
    "n_x = X.shape[0]\n",
    "m = X.shape[1]\n",
    "\n",
    "W1 = np.random.randn(n_hl, n_x) * 0.01\n",
    "b1 = np.zeros((n_hl, 1))\n",
    "W2 = np.random.randn(classes, n_hl) *0.01\n",
    "b2 = np.zeros((classes, 1))\n",
    "\n",
    "for i in range(2000):\n",
    "\n",
    "    #Feed forward\n",
    "    hl_input = np.matmul(W1,X) + b1\n",
    "    hl_output = sigmoid(hl_input)\n",
    "    ol_input = np.matmul(W2,hl_output) + b2\n",
    "    ol_output = softmax(ol_input)\n",
    "\n",
    "    #Calculate the error\n",
    "    cost = multiclass_cross_entropy_loss(Y, ol_output)\n",
    "\n",
    "    #Backpropagation\n",
    "    d_ol_input = ol_output-Y\n",
    "    d_W2 = (1./m) * np.matmul(d_ol_input, hl_output.T)\n",
    "    d_b2 = (1./m) * np.sum(d_ol_input, axis=1, keepdims=True)\n",
    "\n",
    "    d_hl_output = np.matmul(W2.T, d_ol_input)\n",
    "    d_hl_input = d_hl_output * sigmoid_(hl_input)\n",
    "    d_W1 = (1./m) * np.matmul(d_hl_input, X.T)\n",
    "    d_b1 = (1./m) * np.sum(d_hl_input, axis=1, keepdims=True)\n",
    "\n",
    "    #Atualização dos pesos e biases\n",
    "    W2 = W2 - learning_rate * d_W2\n",
    "    b2 = b2 - learning_rate * d_b2\n",
    "    W1 = W1 - learning_rate * d_W1\n",
    "    b1 = b1 - learning_rate * d_b1\n",
    "\n",
    "    if (i % 100 == 0):\n",
    "        print(\"Epoch\", i, \"cost: \", cost)\n",
    "\n",
    "print(\"Final cost:\", cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (16,24196) (32,1) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-67-a8284449a0c5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mclassification_report\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mZ1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mW1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalized_validation_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mb1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mA1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mZ1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mZ2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mW2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mA1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mb2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (16,24196) (32,1) "
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "Z1 = np.matmul(W1, normalized_validation_df.T) + b1\n",
    "A1 = sigmoid(Z1)\n",
    "Z2 = np.matmul(W2, A1) + b2\n",
    "A2 = softmax(Z2)\n",
    "\n",
    "predictions = np.argmax(A2, axis=0)\n",
    "labels = np.argmax(Y_validation, axis=0)\n",
    "\n",
    "print(confusion_matrix(predictions, labels))\n",
    "print(classification_report(predictions, labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rede Neural com 2 Camadas Escondidas (Multiclasse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 cost:  2.302549660265978\n",
      "Epoch 100 cost:  2.195334733023653\n",
      "Epoch 200 cost:  1.1619762652462717\n",
      "Epoch 300 cost:  0.8131110830503772\n",
      "Epoch 400 cost:  0.6411048256936106\n",
      "Epoch 500 cost:  0.5474594263480588\n",
      "Epoch 600 cost:  0.5306064928346175\n",
      "Epoch 700 cost:  0.5092909280294695\n",
      "Epoch 800 cost:  0.4532357925252109\n",
      "Epoch 900 cost:  0.4286813327062372\n",
      "Epoch 1000 cost:  0.39472363794750415\n",
      "Epoch 1100 cost:  0.44625788917048936\n",
      "Epoch 1200 cost:  0.3856313005201653\n",
      "Epoch 1300 cost:  0.36506972377102165\n",
      "Epoch 1400 cost:  0.3964792644353302\n",
      "Epoch 1500 cost:  0.3650909233731397\n",
      "Epoch 1600 cost:  0.3487465770139985\n",
      "Epoch 1700 cost:  0.35048209766329125\n",
      "Epoch 1800 cost:  0.39848935299425026\n",
      "Epoch 1900 cost:  0.3341025107260855\n",
      "Final cost: 0.3230119799774802\n"
     ]
    }
   ],
   "source": [
    "#ONE HIDDEN LAYER\n",
    "\n",
    "classes = 10\n",
    "n_hl_1 = 64\n",
    "n_hl_2 = 64\n",
    "learning_rate = 0.4\n",
    "\n",
    "df = train_df.T\n",
    "Y = Y_train\n",
    "\n",
    "n_x = df.shape[0]\n",
    "m = df.shape[1]\n",
    "\n",
    "weights_1 = np.random.randn(n_hl_1, n_x) * 0.01\n",
    "b1 = np.zeros((n_hl_1, 1))\n",
    "weights_2 = np.random.randn(n_hl_2, n_hl_1) *0.01\n",
    "b2 = np.zeros((n_hl_2, 1))\n",
    "weights_3 = np.random.randn(classes, n_hl_2) *0.01\n",
    "b3 = np.zeros((classes, 1))\n",
    "\n",
    "for i in range(2000):\n",
    "\n",
    "    #Feed forward\n",
    "    hl_1_input = np.matmul(weights_1,df) + b1\n",
    "    hl_1_output = relu(hl_1_input) \n",
    "    hl_2_input = np.matmul(weights_2,hl_1_output) + b2\n",
    "    hl_2_output = relu(hl_2_input)\n",
    "    ol_input = np.matmul(weights_3,hl_2_output) + b3\n",
    "    ol_output = softmax(ol_input)\n",
    "\n",
    "    #Calculate the error\n",
    "    cost = multiclass_cross_entropy_loss(Y, ol_output)\n",
    "\n",
    "    #Backpropagation\n",
    "    d_ol_input = ol_output-Y\n",
    "    d_weights_3 = (1./m) * np.matmul(d_ol_input, hl_2_output.T)\n",
    "    d_b3 = (1./m) * np.sum(d_ol_input, axis=1, keepdims=True)\n",
    "    \n",
    "    d_hl_2_output = np.matmul(weights_3.T, d_ol_input)\n",
    "    d_hl_2_input = d_hl_2_output * relu_(hl_2_input)\n",
    "    d_weights_2 = (1./m) * np.matmul(d_hl_2_input, hl_1_output.T)\n",
    "    d_b2 = (1./m) * np.sum(d_hl_2_input, axis=1, keepdims=True)\n",
    "\n",
    "    d_hl_1_output = np.matmul(weights_2.T, d_hl_2_input)\n",
    "    d_hl_1_input = d_hl_1_output * relu_(hl_1_input)\n",
    "    d_weights_1 = (1./m) * np.matmul(d_hl_1_input, df.T)\n",
    "    d_b1 = (1./m) * np.sum(d_hl_1_input, axis=1, keepdims=True)\n",
    "\n",
    "    #Atualização dos pesos e biases\n",
    "    weights_3 = weights_3 - learning_rate * d_weights_3\n",
    "    b3 = b3 - learning_rate * d_b3\n",
    "    weights_2 = weights_2 - learning_rate * d_weights_2\n",
    "    b2 = b2 - learning_rate * d_b2\n",
    "    weights_1 = weights_1 - learning_rate * d_weights_1\n",
    "    b1 = b1 - learning_rate * d_b1\n",
    "\n",
    "    if (i % 100 == 0):\n",
    "        print(\"Epoch\", i, \"cost: \", cost)\n",
    "\n",
    "print(\"Final cost:\", cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2076    5   34   45    2    2  390    0   11    0]\n",
      " [   7 2304    2   17    1    0    3    0    2    0]\n",
      " [  37    6 1754   11  116    0  210    0   15    0]\n",
      " [ 154   68   33 2240   97    1  114    0   21    0]\n",
      " [   7   10  409  105 2083    0  223    0   18    0]\n",
      " [   1    0    0    0    0 2271    1   69   10   27]\n",
      " [ 175    0  136   42  165    1 1441    0   19    0]\n",
      " [   0    0    0    0    0   53    0 2120    3   35]\n",
      " [  26    2   11    3   11    7   25    5 2352    1]\n",
      " [   0    0    0    0    0   42    0  231    0 2278]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.84      0.81      0.82      2565\n",
      "          1       0.96      0.99      0.97      2336\n",
      "          2       0.74      0.82      0.77      2149\n",
      "          3       0.91      0.82      0.86      2728\n",
      "          4       0.84      0.73      0.78      2855\n",
      "          5       0.96      0.95      0.96      2379\n",
      "          6       0.60      0.73      0.66      1979\n",
      "          7       0.87      0.96      0.91      2211\n",
      "          8       0.96      0.96      0.96      2443\n",
      "          9       0.97      0.89      0.93      2551\n",
      "\n",
      "avg / total       0.87      0.86      0.87     24196\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "Z1 = np.matmul(weights_1, normalized_validation_df.T) + b1\n",
    "A1 = relu(Z1)\n",
    "Z2 = np.matmul(weights_2, A1) + b2\n",
    "A2 = relu(Z2)\n",
    "Z3 = np.matmul(weights_3, A2) + b3\n",
    "A3 = softmax(Z3)\n",
    "\n",
    "predictions = np.argmax(A3, axis=0)\n",
    "labels = np.argmax(Y_validation, axis=0)\n",
    "\n",
    "print(confusion_matrix(predictions, labels))\n",
    "print(classification_report(predictions, labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
