{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classificação do Fashion MNIST com NN simples e Regressão Logística"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Funções das Redes Neurais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Criando a funcao de normalização de um dataframe inteiro\n",
    "    input:\n",
    "        df: Dataframe\n",
    "    output:\n",
    "        df: Dataframe com valores normalizados\n",
    "'''\n",
    "def normalize_dataframe(df):\n",
    "    for column in df:\n",
    "        df[column] = df[column]/255\n",
    "    return df\n",
    "\n",
    "\n",
    "'''Funcao para aplicar relu em um vetor\n",
    "    input:\n",
    "        z: weights * params + bias\n",
    "    output:\n",
    "        s: 0 para valores negativos z para valores positivos aplicado em cada amostra\n",
    "'''    \n",
    "def relu(z):\n",
    "    return np.maximum(z, 0)\n",
    "\n",
    "\n",
    "'''Funcao para aplicar a derivada da relu em um vetor\n",
    "    input:\n",
    "        z: weights * params + bias\n",
    "    output:\n",
    "        s: 0 para valores negativos 1 para valores positivos aplicado em cada amostra\n",
    "'''    \n",
    "def relu_(z):\n",
    "    return np.array(z>0).astype(int)\n",
    "\n",
    "\n",
    "'''Funcao para aplicar sigmoid em um vetor\n",
    "    input:\n",
    "        z: weights * params + bias\n",
    "    output:\n",
    "        s: valor entre 0-1 da aplicação da função para cada amostra\n",
    "'''\n",
    "def sigmoid(z):\n",
    "    s = 1 / (1 + np.exp(-z))\n",
    "    return s\n",
    "\n",
    "'''Funcao para aplicar a derivada da sigmoid em um vetor\n",
    "    input:\n",
    "        z: weights * params + bias\n",
    "    output:\n",
    "        p: taxa de variação da sigmoid para cada amostra\n",
    "'''\n",
    "def sigmoid_(z):\n",
    "    s = sigmoid(Z1) * (1 - sigmoid(Z1))\n",
    "    return s\n",
    "\n",
    "\n",
    "'''Funcao para calcular a tangente hiporbólica em um vetor\n",
    "    input:\n",
    "        x: weights * params + bias\n",
    "    output:\n",
    "        s: valor entre 0-1 da aplicação da função para cada amostra\n",
    "'''\n",
    "def tanh(x):\n",
    "    s = (2 / (1 + np.exp(-2*x))) -1\n",
    "    return s\n",
    "\n",
    "'''Funcao para calcular a derivada da tangente hiporbólica em um vetor\n",
    "    input:\n",
    "        x: weights * params + bias\n",
    "    output:\n",
    "        s: porcentagem de chance do valor ser da classe predita para cada amostra\n",
    "'''\n",
    "def tanh_(x):\n",
    "    s = 1 - tanh(x)**2\n",
    "    return s\n",
    "\n",
    "\n",
    "'''Funcao para calcular o erro médio da predição em todo o dataset utilizando cross-entropy (Não pode ser utilizada com Relu)\n",
    "    input:\n",
    "        Y: labels do dataset\n",
    "        Y_pred: labels preditas\n",
    "    output:\n",
    "        custo: erro médio da predição\n",
    "'''\n",
    "def cross_entropy_loss(Y, Y_pred):\n",
    "    m = Y.shape[1]\n",
    "    custo = -(1./m) * ( np.sum( np.multiply(np.log(Y_pred),Y) ) + np.sum( np.multiply(np.log(1-Y_pred),(1-Y)) ) )\n",
    "    return custo\n",
    "\n",
    "\n",
    "'''Funcao para calcular o erro médio da predição em todo o dataset utilizando custo quadratico\n",
    "    input:\n",
    "        x: weights * params + bias\n",
    "    output:\n",
    "        p: porcentagem de chance do valor ser da classe predita\n",
    "'''\n",
    "def quadratic_loss(Y, Y_pred):\n",
    "        m = Y.shape[1]\n",
    "        return (1./m)*0.5*np.linalg.norm(Y_pred-Y)**2\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparando os Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import time\n",
    "\n",
    "df = pd.read_csv(\"data/fashion-mnist_train.csv\")\n",
    "\n",
    "#Separate the Training DF into Train and Validation\n",
    "msk = np.random.rand(len(df)) < 0.6\n",
    "\n",
    "train_df = df[msk]\n",
    "validation_df = df[~msk]\n",
    "\n",
    "Y_train = train_df[\"label\"]\n",
    "Y_validation = validation_df[\"label\"]\n",
    "\n",
    "train_df = train_df.loc[:, train_df.columns != \"label\"]\n",
    "validation_df = validation_df.loc[:, validation_df.columns != \"label\"]\n",
    "\n",
    "#test_df = pd.read_csv(\"data/fashion-mnist_test.csv\")\n",
    "\n",
    "normalized_train_df = normalize_dataframe(train_df)\n",
    "normalized_validation_df = normalize_dataframe(validation_df)\n",
    "\n",
    "\n",
    "## A sessão abaixo será apagada. Só é utilizada porque falta a SOFTMAX na ultima camada\n",
    "#Put labels to single class\n",
    "\n",
    "y_aux = np.zeros(Y_train.shape)\n",
    "y_aux[np.where(Y_train == 0.0)[0]] = 1\n",
    "Y_train = y_aux\n",
    "\n",
    "print(Y_train.max())\n",
    "print(Y_train.min())\n",
    "\n",
    "y_aux = np.zeros(Y_validation.shape)\n",
    "y_aux[np.where(Y_validation == 0.0)[0]] = 1\n",
    "Y_validation = y_aux"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualização dos Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "i = 3\n",
    "#print(np.array(train_df.iloc[1]).reshape(28,28))\n",
    "plt.imshow(np.array(train_df.iloc[5]).reshape(28,28), cmap = matplotlib.cm.binary)\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "print(Y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rede Neural sem Camadas Escondidas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 cost:  0.051135740978545015\n",
      "Epoch 100 cost:  0.03893971457554921\n",
      "Epoch 200 cost:  0.037636272280184915\n",
      "Epoch 300 cost:  0.03693008125617631\n",
      "Epoch 400 cost:  0.03649006161015847\n",
      "Epoch 500 cost:  0.0361874562817741\n",
      "Epoch 600 cost:  0.03595700352197947\n",
      "Epoch 700 cost:  0.035772829636446166\n",
      "Epoch 800 cost:  0.035618628376356765\n",
      "Epoch 900 cost:  0.03548481175011885\n",
      "Epoch 1000 cost:  0.03536993437382601\n",
      "Epoch 1100 cost:  0.03526920738570392\n",
      "Epoch 1200 cost:  0.03517931813008395\n",
      "Epoch 1300 cost:  0.035098168242733485\n",
      "Epoch 1400 cost:  0.03502336391538415\n",
      "Epoch 1500 cost:  0.03495465263458976\n",
      "Epoch 1600 cost:  0.03489183529125428\n",
      "Epoch 1700 cost:  0.0348335203957646\n",
      "Epoch 1800 cost:  0.03477933718136989\n",
      "Epoch 1900 cost:  0.03472895774734202\n",
      "Final cost: 0.03641135985981767\n"
     ]
    }
   ],
   "source": [
    "#NO HIDDEN LAYER\n",
    "\n",
    "learning_rate = 0.1\n",
    "\n",
    "X = normalized_train_df.T\n",
    "Y = np.array(Y_train).reshape(1, X.shape[1])\n",
    "\n",
    "n_x = X.shape[0]\n",
    "m = X.shape[1]\n",
    "\n",
    "W = np.random.randn(n_x, 1) * 0.01\n",
    "b = np.zeros((1, 1))\n",
    "\n",
    "for i in range(2000):\n",
    "    Z = np.matmul(W.T, X) + b\n",
    "    A = relu(Z)\n",
    "\n",
    "    cost = quadratic_loss(Y, A)\n",
    "\n",
    "    dW = (1/m) * np.matmul(X, (A-Y).T)\n",
    "    db = (1/m) * np.sum(A-Y, axis=1, keepdims=True)\n",
    "\n",
    "    W = W - learning_rate * dW\n",
    "    b = b - learning_rate * db\n",
    "\n",
    "    if (i % 100 == 0):\n",
    "        print(\"Epoch\", i, \"cost: \", cost)\n",
    "\n",
    "print(\"Final cost:\", cost)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "\n",
    "X_validation = normalized_validation_df.T\n",
    "Y_validation = np.array(Y_validation).reshape(1, X_validation.shape[1])\n",
    "\n",
    "n_x = X_validation.shape[0]\n",
    "m = X_validation.shape[1]\n",
    "Z = np.matmul(W.T, X_validation) + b\n",
    "A = sigmoid(Z)\n",
    "\n",
    "predictions = (A>.5)[0,:]\n",
    "labels = (Y_validation == 1)[0,:]\n",
    "\n",
    "print(confusion_matrix(predictions, labels))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rede Neural com 1 Camada Escondida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 cost:  0.05148921429381009\n",
      "Epoch 100 cost:  0.024321304995056515\n",
      "Epoch 200 cost:  0.022435680888164403\n",
      "Epoch 300 cost:  0.021395930507784964\n",
      "Epoch 400 cost:  0.020522833481296944\n",
      "Epoch 500 cost:  0.019666102625197062\n",
      "Epoch 600 cost:  0.018918617974133675\n",
      "Epoch 700 cost:  0.018296930192860345\n",
      "Epoch 800 cost:  0.0220776141038961\n",
      "Epoch 900 cost:  0.017397535890385653\n",
      "Epoch 1000 cost:  0.01708932705717457\n",
      "Epoch 1100 cost:  0.0168397401752846\n",
      "Epoch 1200 cost:  0.016635983643505832\n",
      "Epoch 1300 cost:  0.017114589675492307\n",
      "Epoch 1400 cost:  0.016370817504186345\n",
      "Epoch 1500 cost:  0.016165625398442754\n",
      "Epoch 1600 cost:  0.016045986176954085\n",
      "Epoch 1700 cost:  0.01597662292429264\n",
      "Epoch 1800 cost:  0.016782207091304333\n",
      "Epoch 1900 cost:  0.01579291146866112\n",
      "Final cost: 0.01564525516238836\n"
     ]
    }
   ],
   "source": [
    "#ONE HIDDEN LAYER\n",
    "\n",
    "learning_rate = 0.1\n",
    "\n",
    "X = normalized_train_df.T\n",
    "Y = np.array(Y_train).reshape(1, X.shape[1])\n",
    "\n",
    "n_x = X.shape[0]\n",
    "n_h = 64\n",
    "m = X.shape[1]\n",
    "\n",
    "W1 = np.random.randn(n_h, n_x) * 0.01\n",
    "b1 = np.zeros((n_h, 1))\n",
    "W2 = np.random.randn(1, n_h) * 0.01\n",
    "b2 = np.zeros((1, 1))\n",
    "\n",
    "for i in range(2000):\n",
    "\n",
    "    Z1 = np.matmul(W1, X) + b1\n",
    "    A1 = tanh(Z1)\n",
    "    Z2 = np.matmul(W2, A1) + b2\n",
    "    A2 = tanh(Z2)\n",
    "\n",
    "    cost = quadratic_loss(Y, A2)\n",
    "\n",
    "    dZ2 = A2-Y\n",
    "    dW2 = (1./m) * np.matmul(dZ2, A1.T)\n",
    "    db2 = (1./m) * np.sum(dZ2, axis=1, keepdims=True)\n",
    "\n",
    "    dA1 = np.matmul(W2.T, dZ2)\n",
    "    dZ1 = dA1 * tanh_(Z1)\n",
    "    dW1 = (1./m) * np.matmul(dZ1, X.T)\n",
    "    db1 = (1./m) * np.sum(dZ1, axis=1, keepdims=True)\n",
    "\n",
    "    W2 = W2 - learning_rate * dW2\n",
    "    b2 = b2 - learning_rate * db2\n",
    "    W1 = W1 - learning_rate * dW1\n",
    "    b1 = b1 - learning_rate * db1\n",
    "\n",
    "    if i % 100 == 0:\n",
    "        print(\"Epoch\", i, \"cost: \", cost)\n",
    "\n",
    "print(\"Final cost:\", cost)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[21175   586]\n",
      " [  391  1730]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      False       0.98      0.97      0.98     21761\n",
      "       True       0.75      0.82      0.78      2121\n",
      "\n",
      "avg / total       0.96      0.96      0.96     23882\n",
      "\n",
      "[[20636   234]\n",
      " [  930  2082]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "\n",
    "X_validation = normalized_validation_df.T\n",
    "Y_validation = np.array(Y_validation).reshape(1, X_validation.shape[1])\n",
    "\n",
    "n_x = X_validation.shape[0]\n",
    "m = X_validation.shape[1]\n",
    "Z = np.matmul(W.T, X_validation) + b\n",
    "A = tanh(Z)\n",
    "\n",
    "Z1 = np.matmul(W1, X_validation) + b1\n",
    "A1 = tanh(Z1)\n",
    "Z2 = np.matmul(W2, A1) + b2\n",
    "A2 = tanh(Z2)\n",
    "\n",
    "predictions = (A2>.5)[0,:]\n",
    "labels = (Y_validation == 1)[0,:]\n",
    "\n",
    "print(confusion_matrix(predictions, labels))\n",
    "print(classification_report(predictions, labels))\n",
    "\n",
    "predictions = (A>.5)[0,:]\n",
    "labels = (Y_validation == 1)[0,:]\n",
    "\n",
    "print(confusion_matrix(predictions, labels))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
