{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classificação do Fashion MNIST com NN simples e Regressão Logística"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Funções das Redes Neurais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "'''Criando a funcao de normalização de um dataframe inteiro\n",
    "    input:\n",
    "        df: Dataframe\n",
    "    output:\n",
    "        df: Dataframe com valores normalizados\n",
    "'''\n",
    "def normalize_dataframe(df):\n",
    "    for column in df:\n",
    "        df[column] = df[column]/255\n",
    "    return df\n",
    "\n",
    "\n",
    "'''Funcao para aplicar relu em um vetor\n",
    "    input:\n",
    "        z: weights * params + bias\n",
    "    output:\n",
    "        s: 0 para valores negativos z para valores positivos aplicado em cada amostra\n",
    "'''    \n",
    "def relu(z):\n",
    "    return np.maximum(z, 0)\n",
    "\n",
    "\n",
    "'''Funcao para aplicar a derivada da relu em um vetor\n",
    "    input:\n",
    "        z: weights * params + bias\n",
    "    output:\n",
    "        s: 0 para valores negativos 1 para valores positivos aplicado em cada amostra\n",
    "'''    \n",
    "def relu_(z):\n",
    "    return np.array(z>0).astype(int)\n",
    "\n",
    "\n",
    "'''Funcao para aplicar sigmoid em um vetor\n",
    "    input:\n",
    "        z: weights * params + bias\n",
    "    output:\n",
    "        s: valor entre 0-1 da aplicação da função para cada amostra\n",
    "'''\n",
    "def sigmoid(z):\n",
    "    #print(z.shape)\n",
    "    s = 1 / (1 + np.exp(-z))\n",
    "    return s\n",
    "\n",
    "\n",
    "'''Funcao para aplicar a derivada da sigmoid em um vetor\n",
    "    input:\n",
    "        z: weights * params + bias\n",
    "    output:\n",
    "        p: taxa de variação da sigmoid para cada amostra\n",
    "'''\n",
    "def sigmoid_(z):\n",
    "    #print(sigmoid(Z1).shape)\n",
    "    s = sigmoid(z) * (1 - sigmoid(z))\n",
    "    return s\n",
    "\n",
    "\n",
    "'''Funcao para calcular a tangente hiporbólica em um vetor\n",
    "    input:\n",
    "        x: weights * params + bias\n",
    "    output:\n",
    "        s: valor entre 0-1 da aplicação da função para cada amostra\n",
    "'''\n",
    "def tanh(x):\n",
    "    s = (2 / (1 + np.exp(-2*x))) -1\n",
    "    return s\n",
    "\n",
    "\n",
    "'''Funcao para calcular a derivada da tangente hiporbólica em um vetor\n",
    "    input:\n",
    "        x: weights * params + bias\n",
    "    output:\n",
    "        s: porcentagem de chance do valor ser da classe predita para cada amostra\n",
    "'''\n",
    "def tanh_(x):\n",
    "    s = 1 - tanh(x)**2\n",
    "    return s\n",
    "\n",
    "\n",
    "'''Funcao para calcular o erro médio da predição utilizando cross-entropy (Não pode ser utilizada com Relu, mas ok na output com softmax)\n",
    "    input:\n",
    "        Y: Labels\n",
    "        Y_pred: Labels Preditas\n",
    "    output:\n",
    "        custo: erro médio da predição\n",
    "'''\n",
    "def cross_entropy_loss(Y, Y_pred):\n",
    "    samples_number = Y.shape[1]\n",
    "    custo = -(1./samples_number) * ( np.sum( np.multiply(np.log(Y_pred),Y) ) + np.sum( np.multiply(np.log(1-Y_pred),(1-Y)) ) )\n",
    "    return custo\n",
    "\n",
    "\n",
    "'''Funcao para calcular o erro médio da predição utilizando cross-entropy multiclasse (Não pode ser utilizada com Relu, mas ok na output com softmax)\n",
    "    input:\n",
    "        Y: Labels\n",
    "        Y_pred: Labels Preditas\n",
    "    output:\n",
    "        custo: erro médio da predição\n",
    "'''\n",
    "def multiclass_cross_entropy_loss(Y, Y_pred):\n",
    "\n",
    "    aux = np.sum(np.multiply(Y, np.log(Y_pred)))\n",
    "    samples_number = Y.shape[1]\n",
    "    custo = -(1/samples_number) * aux\n",
    "\n",
    "    return custo\n",
    "    \n",
    "\n",
    "'''Funcao para calcular a probabilidade de ser de cada classe\n",
    "    input:\n",
    "        z: entrada da camada de saída\n",
    "    output:\n",
    "        p: probabilidade de ser de cada classe\n",
    "'''\n",
    "def softmax(z):\n",
    "    return np.exp(z) / np.sum(np.exp(z), axis=0)\n",
    "\n",
    "\n",
    "'''Funcao para calcular a matriz de confusão\n",
    "    input:\n",
    "        Y: labels corretas\n",
    "        Y_pred: labels preditas\n",
    "        classes: numéro de classes\n",
    "    output:\n",
    "        confusion_matrix: matriz de confusão\n",
    "'''\n",
    "def get_confusion_matrix(Y, Y_pred, classes):\n",
    "    \n",
    "    if(Y_pred.max() != 1):\n",
    "        Y_pred = binarize_labels(Y_pred)\n",
    "    \n",
    "    confusion_matrix = np.zeros((classes,classes)).astype(int)\n",
    "    \n",
    "    for y in range (Y.shape[1]):\n",
    "        for c_pred in range (classes):\n",
    "            if(Y_pred[c_pred, y] == 1):\n",
    "                for c in range (classes):\n",
    "                    if(Y[c, y] == 1):\n",
    "                        confusion_matrix[c, c_pred]+=1\n",
    "    \n",
    "    return confusion_matrix\n",
    "\n",
    "'''Funcao para converter um vetor de probabilidades de classes em one-hot-encoding\n",
    "    input:\n",
    "        L: vetor de probabilidade das labels\n",
    "    output:\n",
    "        Y_pred: vetor de labels (one-hot-encoding) \n",
    "    Ex: \n",
    "        Y_pred[:,1] = [0,1,0,0,0,0,0,0,0,0] quer dizer que a classe da sample 1 é 2\n",
    "'''\n",
    "def binarize_labels(L):\n",
    "    samples_number = L.shape[1]\n",
    "    classes = L.shape[0]\n",
    "    \n",
    "    Y_pred = np.argmax(L, axis=0).reshape(1, samples_number)\n",
    "\n",
    "    y_aux = np.eye(classes)[Y_pred.astype('int32')]\n",
    "    y_aux = y_aux.T.reshape(classes, samples_number)\n",
    "\n",
    "    Y_pred = y_aux\n",
    "    \n",
    "    return Y_pred\n",
    "\n",
    "'''Funcao para calcular a predição (Feed forward) de um dataframe dados parâmetros da rede neural com 1 camada escondida\n",
    "    input:\n",
    "        weights_1: vetor de pesos para calculo da entrada da camada escondida\n",
    "        weights_2: vetor de pesos para calculo da entrada da camada de saída\n",
    "        b1: vetor de pesos do bias para calculo da entrada da camada escondida\n",
    "        b2: vetor de pesos do bias para calculo da entrada da camada de saída\n",
    "        df: Dataframe a ser predito\n",
    "        activation_function: qual função de ativação será usada nos neurônios (relu, sigmoid, tanh)\n",
    "    output:\n",
    "        ol_output: vetor contendo as labels preditas\n",
    "'''\n",
    "def predict_labels_1_hidden(weights_1, weights_2, b1, b2, df, activation_function):\n",
    "    \n",
    "    hl_input = np.matmul(weights_1,df.T) + b1\n",
    "    hl_output = activation_function(hl_input)\n",
    "    ol_input = np.matmul(weights_2,hl_output) + b2\n",
    "    ol_output = softmax(ol_input)\n",
    "    \n",
    "    return ol_output\n",
    "\n",
    "'''Funcao para calcular a predição (Feed forward) de um dataframe dados parâmetros de uma rede neural com 2 camadas escondidas\n",
    "    input:\n",
    "        weights_1: vetor de pesos para calculo da entrada da primeira camada escondida\n",
    "        weights_2: vetor de pesos para calculo da entrada da segunda camada escondida\n",
    "        weights_2: vetor de pesos para calculo da entrada da camada de saída\n",
    "        b1: vetor de pesos do bias para calculo da entrada da primeira camada escondida\n",
    "        b1: vetor de pesos do bias para calculo da entrada da segunda camada escondida\n",
    "        b2: vetor de pesos do bias para calculo da entrada da camada de saída\n",
    "        df: Dataframe a ser predito\n",
    "        activation_function: qual função de ativação será usada nos neurônios (relu, sigmoid, tanh)\n",
    "    output:\n",
    "        ol_output: vetor contendo as labels preditas\n",
    "'''\n",
    "def predict_labels_2_hidden(weights_1, weights_2, weights_3, b1, b2, b3, df, activation_function):\n",
    "    \n",
    "    hl_1_input = np.matmul(weights_1,normalized_validation_df.T) + b1\n",
    "    hl_1_output = activation_function(hl_1_input) \n",
    "    hl_2_input = np.matmul(weights_2,hl_1_output) + b2\n",
    "    hl_2_output = activation_function(hl_2_input)\n",
    "    ol_input = np.matmul(weights_3,hl_2_output) + b3\n",
    "    ol_output = softmax(ol_input)\n",
    "    \n",
    "    return ol_output\n",
    "\n",
    "'''Funcao para obter as métricas de performance: precision, recall e f1_score\n",
    "    input:\n",
    "        confusion_matrix: matriz de confusão (use get_confusion_matrix() para calculá-la)\n",
    "    output:\n",
    "        precision: relação entre a quantidade de positivos preditos pela quantidade real de positivos \n",
    "        recall: relação entre a quantidade de positivos esperados pela quantidade de positivos preditos\n",
    "        f1_score: metrica para relacionar precision e recall em uma única métrica\n",
    "'''\n",
    "def get_metrics(confusion_matrix):\n",
    "    precision = get_precision(confusion_matrix)\n",
    "    recall = get_recall(confusion_matrix)\n",
    "    f1_score = get_f1_score(precision, recall)\n",
    "    \n",
    "    return precision, recall, f1_score\n",
    "\n",
    "def print_metrics(precision, recall, f1_score):\n",
    "    precision = np.around(precision, decimals=2).reshape( precision.shape[0],1)\n",
    "    recall = np.around(recall, decimals=2).reshape( recall.shape[0],1)\n",
    "    f1_score = np.around(f1_score, decimals=2).reshape( f1_score.shape[0],1)\n",
    "    \n",
    "    average_precision = np.sum(precision)/precision.shape[0]\n",
    "    average_recall = np.sum(recall)/recall.shape[0]\n",
    "    average_f1_score = np.sum(f1_score)/f1_score.shape[0]\n",
    "    \n",
    "    print(\"\\n\\nPrecision (Pr), Recall (Re) and F1_Score (F1) of each class: \")\n",
    "    print(\"Pr   Re    F1\")\n",
    "    print(re.sub(r' *\\n *', '\\n', np.array_str(np.c_[precision, recall, f1_score]).replace('[', '').replace(']', '').strip()))\n",
    "    \n",
    "    print(\"\\n\\nAverage Precision: \", round(average_precision,2) , \"\\nAverage Recall: \", round(average_recall, 2) ,\"\\nAverage F1_Score: \", round(average_f1_score,2))\n",
    "    \n",
    "    #print(precision,\" \", recall,\" \", f1_score)\n",
    "\n",
    "def get_recall(confusion_matrix):\n",
    "    precision = np.ones((confusion_matrix.shape[0]))\n",
    "    for i in range (confusion_matrix.shape[0]):\n",
    "        tp = confusion_matrix[i,i]\n",
    "        tp_fp = np.sum(confusion_matrix[:,i])\n",
    "        \n",
    "        precision[i] = tp/tp_fp\n",
    "        \n",
    "    return precision\n",
    "\n",
    "def get_precision(confusion_matrix):\n",
    "    recall = np.ones(confusion_matrix.shape[0])\n",
    "    \n",
    "    for i in range (confusion_matrix.shape[0]):\n",
    "        tp = confusion_matrix[i,i]\n",
    "        tp_fn = np.sum(confusion_matrix[i,:])\n",
    "        \n",
    "        recall[i] = tp/tp_fn\n",
    "        \n",
    "    return recall\n",
    "\n",
    "def get_f1_score(precision, recall):\n",
    "    f1 = 2*( (precision*recall) / (precision+recall) )\n",
    "    return f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparando os Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/fashion-mnist_train.csv\")\n",
    "\n",
    "#Separate the Training DF into Train and Validation\n",
    "msk = np.random.rand(len(df)) < 0.7 \n",
    "\n",
    "train_df = df[msk]\n",
    "validation_df = df[~msk]\n",
    "\n",
    "Y_train = train_df[\"label\"]\n",
    "Y_validation = validation_df[\"label\"]\n",
    "\n",
    "train_df = train_df.loc[:, train_df.columns != \"label\"]\n",
    "validation_df = validation_df.loc[:, validation_df.columns != \"label\"]\n",
    "\n",
    "#test_df = pd.read_csv(\"data/fashion-mnist_test.csv\")\n",
    "\n",
    "normalized_train_df = normalize_dataframe(train_df)\n",
    "normalized_validation_df = normalize_dataframe(validation_df)\n",
    "\n",
    "\n",
    "#One hot encoding labels para o softmax\n",
    "classes = 10\n",
    "\n",
    "samples_train = Y_train.shape[0]\n",
    "samples_validation = Y_validation.shape[0]\n",
    "\n",
    "Y_train = np.array(Y_train).reshape(1, samples_train)\n",
    "\n",
    "y_aux = np.eye(classes)[Y_train.astype('int32')]\n",
    "y_aux = y_aux.T.reshape(classes, samples_train)\n",
    "Y_train = y_aux\n",
    "\n",
    "Y_validation = np.array(Y_validation).reshape(1, samples_validation)\n",
    "\n",
    "y_aux = np.eye(classes)[Y_validation.astype('int32')]\n",
    "y_aux = y_aux.T.reshape(classes, samples_validation)\n",
    "\n",
    "Y_validation = y_aux"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualização dos Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "i = 3\n",
    "#print(np.array(train_df.iloc[1]).reshape(28,28))\n",
    "plt.imshow(np.array(train_df.iloc[5]).reshape(28,28), cmap = matplotlib.cm.binary)\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "print(Y_train[:,5])\n",
    "\n",
    "# 0 \tT-shirt/top\n",
    "# 1 \tTrouser\n",
    "# 2 \tPullover\n",
    "# 3 \tDress\n",
    "# 4 \tCoat\n",
    "# 5 \tSandal\n",
    "# 6 \tShirt\n",
    "# 7 \tSneaker\n",
    "# 8 \tBag\n",
    "# 9 \tAnkle boot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rede Neural com 1 Camada Escondida (Multi Class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 cost:  2.3023500365842176\n",
      "Epoch 100 cost:  0.6466817799436761\n",
      "Epoch 200 cost:  0.5209745789867123\n",
      "Epoch 300 cost:  0.46108273038976627\n",
      "Epoch 400 cost:  0.4343542903639904\n",
      "Epoch 500 cost:  0.41040765750593705\n",
      "Epoch 600 cost:  0.3974127542991597\n",
      "Epoch 700 cost:  0.3816129761515525\n",
      "Epoch 800 cost:  0.37499823321922643\n",
      "Epoch 900 cost:  0.3663938488742101\n",
      "Epoch 1000 cost:  0.37220201467984626\n",
      "Epoch 1100 cost:  0.3498382159280077\n",
      "Epoch 1200 cost:  0.33780166970881115\n",
      "Epoch 1300 cost:  0.3361343191244013\n",
      "Epoch 1400 cost:  0.32991720415453213\n",
      "Epoch 1500 cost:  0.3457432832149773\n",
      "Epoch 1600 cost:  0.3192323317125567\n",
      "Epoch 1700 cost:  0.3213761133865131\n",
      "Epoch 1800 cost:  0.3111745785768754\n",
      "Epoch 1900 cost:  0.31215874783348047\n",
      "Epoch 2000 cost:  0.30537535283342143\n",
      "Epoch 2100 cost:  0.3249095345642264\n",
      "Epoch 2200 cost:  0.32234054825403874\n",
      "Epoch 2300 cost:  0.2954987644480395\n",
      "Epoch 2400 cost:  0.2887034237090432\n",
      "Epoch 2500 cost:  0.2888062970430627\n",
      "Epoch 2600 cost:  0.30027850980597404\n",
      "Epoch 2700 cost:  0.2836074122762552\n",
      "Epoch 2800 cost:  0.2837006083594357\n",
      "Epoch 2900 cost:  0.27942058928243185\n",
      "Final cost: 0.2846337589064495\n"
     ]
    }
   ],
   "source": [
    "#ONE HIDDEN LAYER\n",
    "\n",
    "classes = 10\n",
    "n_hl = 32\n",
    "learning_rate = 0.4\n",
    "\n",
    "# Can be relu, sigmoid or tanh\n",
    "activation_function = tanh\n",
    "#This is the derivative of the activation function denoted by <name>_\n",
    "activation_function_ = tanh_\n",
    "\n",
    "iterations = 3000\n",
    "\n",
    "df = train_df.T\n",
    "Y = Y_train\n",
    "\n",
    "n_x = df.shape[0]\n",
    "m = df.shape[1]\n",
    "\n",
    "weights_1 = np.random.randn(n_hl, n_x) * 0.01\n",
    "b1 = np.zeros((n_hl, 1))\n",
    "weights_2 = np.random.randn(classes, n_hl) *0.01\n",
    "b2 = np.zeros((classes, 1))\n",
    "\n",
    "start_time = time.time()\n",
    "for i in range(iterations):\n",
    "\n",
    "    #Feed forward\n",
    "    hl_input = np.matmul(weights_1,df) + b1\n",
    "    hl_output = activation_function(hl_input)\n",
    "    ol_input = np.matmul(weights_2,hl_output) + b2\n",
    "    ol_output = softmax(ol_input)\n",
    "\n",
    "    #Calculate the error\n",
    "    cost = multiclass_cross_entropy_loss(Y, ol_output)\n",
    "\n",
    "    #Backpropagation\n",
    "    d_ol_input = ol_output-Y\n",
    "    d_weights_2 = (1./m) * np.matmul(d_ol_input, hl_output.T)\n",
    "    d_b2 = (1./m) * np.sum(d_ol_input, axis=1, keepdims=True)\n",
    "\n",
    "    d_hl_output = np.matmul(weights_2.T, d_ol_input)\n",
    "    d_hl_input = d_hl_output * activation_function_(hl_input)\n",
    "    d_weights_1 = (1./m) * np.matmul(d_hl_input, df.T)\n",
    "    d_b1 = (1./m) * np.sum(d_hl_input, axis=1, keepdims=True)\n",
    "\n",
    "    #Atualização dos pesos e biases\n",
    "    weights_2 = weights_2 - learning_rate * d_weights_2\n",
    "    b2 = b2 - learning_rate * d_b2\n",
    "    weights_1 = weights_1 - learning_rate * d_weights_1\n",
    "    b1 = b1 - learning_rate * d_b1\n",
    "\n",
    "    if (i % 100 == 0):\n",
    "        print(\"Epoch\", i, \"cost: \", cost)\n",
    "\n",
    "final_time = time.time() - start_time\n",
    "print(\"Final cost:\", cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1404    6   27   85    3    1  256    0    9    0]\n",
      " [   5 1751    4   37    2    0    5    0    1    0]\n",
      " [  20    2 1480   28   97    1  160    0    9    0]\n",
      " [  38    9   24 1629   41    0   52    0    3    0]\n",
      " [   1    3  262  110 1163    1  215    0    5    0]\n",
      " [   1    1    0    1    0 1707    1   59    6   18]\n",
      " [ 167    4  166   52   68    0 1241    0   20    1]\n",
      " [   0    0    0    0    0   46    0 1707    3   48]\n",
      " [   5    3   10    9    8    6   35    4 1746    1]\n",
      " [   0    0    0    0    0   22    0   59    0 1764]]\n",
      "\n",
      "\n",
      "Precision (Pr), Recall (Re) and F1_Score (F1) of each class: \n",
      "Pr   Re    F1\n",
      "0.78 0.86 0.82\n",
      "0.97 0.98 0.98\n",
      "0.82 0.75 0.79\n",
      "0.91 0.83 0.87\n",
      "0.66 0.84 0.74\n",
      "0.95 0.96 0.95\n",
      "0.72 0.63 0.67\n",
      "0.95 0.93 0.94\n",
      "0.96 0.97 0.96\n",
      "0.96 0.96 0.96\n",
      "\n",
      "\n",
      "Average Precision:  0.87 \n",
      "Average Recall:  0.87 \n",
      "Average F1_Score:  0.87\n"
     ]
    }
   ],
   "source": [
    "prediction = predict_labels_1_hidden(weights_1, weights_2, b1, b2, normalized_validation_df, activation_function)\n",
    "\n",
    "samples_number = prediction.shape[1]\n",
    "\n",
    "\n",
    "predictions = np.argmax(prediction, axis=0)\n",
    "labels = np.argmax(Y_validation, axis=0)\n",
    "\n",
    "confusion_matrix = get_confusion_matrix(Y_validation, prediction, classes)\n",
    "\n",
    "print(confusion_matrix)\n",
    "\n",
    "precision, recall, f1_score = get_metrics(confusion_matrix)\n",
    "\n",
    "print_metrics(precision, recall, f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tempo gasto:  284.22744822502136\n"
     ]
    }
   ],
   "source": [
    "print(\"Tempo gasto: \", final_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rede Neural com 2 Camadas Escondidas (Multiclasse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 cost:  2.303016843874563\n",
      "Epoch 100 cost:  2.3025513680051226\n",
      "Epoch 200 cost:  2.3025486510963717\n",
      "Epoch 300 cost:  2.3025454391016553\n",
      "Epoch 400 cost:  2.302541376544952\n",
      "Epoch 500 cost:  2.3025359002303225\n",
      "Epoch 600 cost:  2.302528030424305\n",
      "Epoch 700 cost:  2.3025159025503203\n",
      "Epoch 800 cost:  2.302495589502313\n",
      "Epoch 900 cost:  2.302457691900237\n",
      "Epoch 1000 cost:  2.3023754158212735\n",
      "Epoch 1100 cost:  2.3021498922497887\n",
      "Epoch 1200 cost:  2.3012244721459107\n",
      "Epoch 1300 cost:  2.292464718852923\n",
      "Epoch 1400 cost:  2.0023170383722952\n",
      "Epoch 1500 cost:  1.7088994240566162\n",
      "Epoch 1600 cost:  1.647634869668965\n",
      "Epoch 1700 cost:  1.525792239102038\n",
      "Epoch 1800 cost:  1.409763848751535\n",
      "Epoch 1900 cost:  1.32776768148217\n",
      "Epoch 2000 cost:  1.25429083470182\n",
      "Epoch 2100 cost:  1.1894576565658566\n",
      "Epoch 2200 cost:  1.1327555990310891\n",
      "Epoch 2300 cost:  1.08195744127886\n",
      "Epoch 2400 cost:  1.0361304359236627\n",
      "Epoch 2500 cost:  0.9938090998549023\n",
      "Epoch 2600 cost:  0.9526539723085957\n",
      "Epoch 2700 cost:  0.9108040866712118\n",
      "Epoch 2800 cost:  0.8684114221632575\n",
      "Epoch 2900 cost:  0.8248933893010189\n",
      "Final cost: 0.7792768059459336\n"
     ]
    }
   ],
   "source": [
    "#ONE HIDDEN LAYER\n",
    "\n",
    "classes = 10\n",
    "n_hl_1 = 16\n",
    "n_hl_2 = 16\n",
    "learning_rate = 0.3\n",
    "\n",
    "activation_function = sigmoid\n",
    "activation_function_ = sigmoid_\n",
    "\n",
    "iterations = 3000\n",
    "\n",
    "df = train_df.T\n",
    "Y = Y_train\n",
    "\n",
    "n_x = df.shape[0]\n",
    "m = df.shape[1]\n",
    "\n",
    "weights_1 = np.random.randn(n_hl_1, n_x) * 0.01\n",
    "b1 = np.zeros((n_hl_1, 1))\n",
    "weights_2 = np.random.randn(n_hl_2, n_hl_1) *0.01\n",
    "b2 = np.zeros((n_hl_2, 1))\n",
    "weights_3 = np.random.randn(classes, n_hl_2) *0.01\n",
    "b3 = np.zeros((classes, 1))\n",
    "\n",
    "start_time = time.time()\n",
    "for i in range(iterations):\n",
    "\n",
    "    #Feed forward\n",
    "    hl_1_input = np.matmul(weights_1,df) + b1\n",
    "    hl_1_output = activation_function(hl_1_input) \n",
    "    hl_2_input = np.matmul(weights_2,hl_1_output) + b2\n",
    "    hl_2_output = activation_function(hl_2_input)\n",
    "    ol_input = np.matmul(weights_3,hl_2_output) + b3\n",
    "    ol_output = softmax(ol_input)\n",
    "\n",
    "    #Calculate the error\n",
    "    cost = multiclass_cross_entropy_loss(Y, ol_output)\n",
    "\n",
    "    #Backpropagation\n",
    "    d_ol_input = ol_output-Y\n",
    "    d_weights_3 = (1./m) * np.matmul(d_ol_input, hl_2_output.T)\n",
    "    d_b3 = (1./m) * np.sum(d_ol_input, axis=1, keepdims=True)\n",
    "    \n",
    "    d_hl_2_output = np.matmul(weights_3.T, d_ol_input)\n",
    "    d_hl_2_input = d_hl_2_output * activation_function_(hl_2_input)\n",
    "    d_weights_2 = (1./m) * np.matmul(d_hl_2_input, hl_1_output.T)\n",
    "    d_b2 = (1./m) * np.sum(d_hl_2_input, axis=1, keepdims=True)\n",
    "\n",
    "    d_hl_1_output = np.matmul(weights_2.T, d_hl_2_input)\n",
    "    d_hl_1_input = d_hl_1_output * activation_function_(hl_1_input)\n",
    "    d_weights_1 = (1./m) * np.matmul(d_hl_1_input, df.T)\n",
    "    d_b1 = (1./m) * np.sum(d_hl_1_input, axis=1, keepdims=True)\n",
    "\n",
    "    #Atualização dos pesos e biases\n",
    "    weights_3 = weights_3 - learning_rate * d_weights_3\n",
    "    b3 = b3 - learning_rate * d_b3\n",
    "    weights_2 = weights_2 - learning_rate * d_weights_2\n",
    "    b2 = b2 - learning_rate * d_b2\n",
    "    weights_1 = weights_1 - learning_rate * d_weights_1\n",
    "    b1 = b1 - learning_rate * d_b1\n",
    "\n",
    "    if (i % 100 == 0):\n",
    "        print(\"Epoch\", i, \"cost: \", cost)\n",
    "    \n",
    "final_time = time.time() - start_time\n",
    "\n",
    "print(\"Final cost:\", cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1458    0   10  203    8    1   91    0   20    0]\n",
      " [  16 1698    6   68    0    0   16    0    1    0]\n",
      " [  50    0 1180   20  319    0  204    0   24    0]\n",
      " [ 155   25    8 1450   16    1  132    0    9    0]\n",
      " [   7    2 1016   88  385    0  247    0   15    0]\n",
      " [   0    0    1    2    0 1467    1  182   62   79]\n",
      " [ 446    2  651   69  250    0  258    0   42    1]\n",
      " [   0    0    0    0    0  138    0 1511    4  151]\n",
      " [   3    1   28    7    8   25   32    1 1721    1]\n",
      " [   0    0    0    0    1   23    0  113    0 1708]]\n",
      "\n",
      "\n",
      "Precision (Pr), Recall (Re) and F1_Score (F1) of each class: \n",
      "Pr   Re    F1\n",
      "0.81 0.68 0.74\n",
      "0.94 0.98 0.96\n",
      "0.66 0.41 0.5\n",
      "0.81 0.76 0.78\n",
      "0.22 0.39 0.28\n",
      "0.82 0.89 0.85\n",
      "0.15 0.26 0.19\n",
      "0.84 0.84 0.84\n",
      "0.94 0.91 0.92\n",
      "0.93 0.88 0.9\n",
      "\n",
      "\n",
      "Average Precision:  0.71 \n",
      "Average Recall:  0.7 \n",
      "Average F1_Score:  0.7\n"
     ]
    }
   ],
   "source": [
    "prediction = predict_labels_2_hidden(weights_1, weights_2, weights_3, b1, b2, b3, normalized_validation_df, activation_function)\n",
    "\n",
    "\n",
    "#Z1 = np.matmul(weights_1, normalized_validation_df.T) + b1\n",
    "#A1 = relu(Z1)\n",
    "#Z2 = np.matmul(weights_2, A1) + b2\n",
    "#A2 = relu(Z2)\n",
    "#Z3 = np.matmul(weights_3, A2) + b3\n",
    "#A3 = softmax(Z3)\n",
    "\n",
    "predictions = np.argmax(prediction, axis=0)\n",
    "labels = np.argmax(Y_validation, axis=0)\n",
    "\n",
    "confusion_matrix = get_confusion_matrix(Y_validation, prediction, classes)\n",
    "\n",
    "print(confusion_matrix)\n",
    "\n",
    "precision, recall, f1_score = get_metrics(confusion_matrix)\n",
    "\n",
    "print_metrics(precision, recall, f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tempo gasto:  254.87904691696167\n"
     ]
    }
   ],
   "source": [
    "print(\"Tempo gasto: \", final_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
